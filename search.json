[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/bruin/HW4/index.html",
    "href": "posts/bruin/HW4/index.html",
    "title": "HW4",
    "section": "",
    "text": "N = 101\nepsilon = 0.2\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\n\ndef get_A(N):\n    \"\"\"\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\nget_A(2700)\n\nMemoryError: Unable to allocate 387. TiB for an array with shape (7290000, 7290000) and data type float64"
  },
  {
    "objectID": "posts/bruin/HW4/index.html#matrix-multiplication",
    "href": "posts/bruin/HW4/index.html#matrix-multiplication",
    "title": "HW4",
    "section": "",
    "text": "N = 101\nepsilon = 0.2\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\n\ndef get_A(N):\n    \"\"\"\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\nget_A(2700)\n\nMemoryError: Unable to allocate 387. TiB for an array with shape (7290000, 7290000) and data type float64"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html",
    "href": "posts/bruin/HW2_copy/Untitled.html",
    "title": "HW2",
    "section": "",
    "text": "This activity aims to find the works of actors in a certain movie only under a specific category: “Acting.” Some actors may also hold other positions on other teams, such as a stunt double on “Crew” or an assistant director on “Production,” but this activity is only interested in the “Acting” positions of the “actors” within a specific movie.\nIn the following tutorial, I will be demonstrating the webscraping process using the TMDB sites of two movies: first from the example movie, Harry Potter and the Philosopher’s Stone, and later from my favorite movie, Kill Bill: Vol. 1.\n\n\n\n\nMy scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper.\n\n\n\n\n\nA file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\n\n\n\n\n\n\nThis is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns\n\n\n\n\n\n\n\n\n\nThis bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#part-1-initial-steps",
    "href": "posts/bruin/HW2_copy/Untitled.html#part-1-initial-steps",
    "title": "HW2",
    "section": "",
    "text": "My scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper."
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "href": "posts/bruin/HW2_copy/Untitled.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "title": "HW2",
    "section": "",
    "text": "A file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "href": "posts/bruin/HW2_copy/Untitled.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "title": "HW2",
    "section": "",
    "text": "This is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#data-visualization",
    "href": "posts/bruin/HW2_copy/Untitled.html#data-visualization",
    "title": "HW2",
    "section": "",
    "text": "This bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW1/index.html",
    "href": "posts/bruin/HW1/index.html",
    "title": "HW1",
    "section": "",
    "text": "I begin by importing the necessary libaries and packages to create a database that consists of three tables: temperatures, stations, and countries. The data for these tables are extracted from csv files. The cleaning process involves some of the column names being renamed and column values adjusted accordingly. sqlite3.connect() points to the database location.\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\nfrom plotly.io import write_html\n\n\ndef prepare_df(df):\n    \"\"\"\n    Cleans the dataframe\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    \n    return(df)\n\n# creating a database in the current directory called temps.db\nwith sqlite3.connect(\"temps.db\") as conn: \n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    stations_csv = \"station-metadata.csv\"\n    stations = pd.read_csv(stations_csv)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n    countries = pd.read_csv(countries_url)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n    \n# the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\ndf.to_sql() writes to a specified table, either temperatures, stations, or countries in temps.db. Becuase the tables for the metadata in our database are from pretty small data sets, there is no need to read it in by chunks. if_exists = “replace” ensures that a new piece is added to the table each time rather than being overwritten.\n\n\n\nNow, I will perform some basic queries on the data tables by joining them based on relational information using INNER JOIN (only rows that have common characteristics similar to the intersection of two sets).\nFor example, the id of the stations table corresponds to the id of the temperatures table.\nThe WHERE parameters take the arguments passed in to correspond to the columns of the dataframe.\nInstead of using the cursor for prototyping SQL queries, here I use pd.read_sql_query(cmd, conn) as an easier approach.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns a dataframe of the temperatures at stations specified\n    by latitude, longitude, and country within a range of months and \n    years.    \n    \"\"\"\n    # creating a database in the current directory \n    # inner join (default for pd.merge): shared columns between the two sets\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE T.year &gt;= {year_begin}\n        AND T.year &lt;= {year_end}\n        AND T.month = {month}\n        AND C.name = \"{country}\"\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.rename(columns={\"Name\" : \"Country\"})\n        \n        return df\n        # the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\n\n\n\n\n\nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nThe return value for the India example has 3152 rows. The countries “Name” column has been altered to be “Country.”\n\n\n\n\nI will now address the question:\nHow does the average yearly change in temperature vary within a given country?\n\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\n\ncoef() computes an estimate of the year-over-year average change in temperature in each month at each station using linear regression. The coefficient of Year is an estimate of the yearly change in Temp when regressing Temp against Year.\n\ndef coef(data_group):\n    \"\"\"\n    Returns the first coeffecient of the linear regression model.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nmin_obs refers to the minimum number of years of data for any given station. Because I am only interested in data of at least min_obs years worth of data in the specified month, all other data should be filtered out using df.transform() plus filtering.\nAdditional keyword arguments may also be passed to the scatter_mapbox() to control the colormap and mapbox style.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Returns a figure of the estimated yearly increase in temperature\n    of the latitude against longitude in a specified country. \n    \"\"\"\n    query_climate_df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    query_data_obs = \\\n    query_climate_df[(query_climate_df.groupby(\"NAME\")[\"Year\"].transform(len)) &gt;= min_obs]\n    \n    # first coefficient of a linear regression model at that station is computed using the .apply() method\n    coefs = query_climate_df.groupby([\"NAME\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    merge_data = pd.merge(coefs, query_climate_df, on = \"NAME\")\n    merge_data[\"Estimated Yearly Increase (°C)\"] = \\\n    round(merge_data[0], 3)\n    \n    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\",\n             6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\",\n             11:\"November\", 12:\"December\"}\n    \n    # first argument must be dataframe\n    # lat and lon arguments tell px which columns contain the latitude and longitude coordinates\n    # hover_name : what appears when hovering over the plotted points\n    # height : aspect ratio\n    fig = px.scatter_mapbox(merge_data,\n                           lat = \"LATITUDE\",\n                           lon = \"LONGITUDE\",\n                           hover_name = \"NAME\",\n                           color = \"Estimated Yearly Increase (°C)\",\n                           height = 300,\n                           opacity = .2,\n                            # colorbar is centered at 0 so that the \"middle\" of the\n                            # of the colorbar corresponds to a coefficient of 0\n                           color_continuous_midpoint = 0,\n                           labels = {\"LATITUDE\", \"LONGITUDE\"},\n                           title = \\\n                           f\"\"\"\n                           Estimates of yearly increase in temperature in {months[month]} for stations in {country}, years {year_begin} - {year_end}\n                           \"\"\",\n                           **kwargs)\n    # controls which map tiles are used in the visualization and the amount of whitespace\n    fig.update_layout(margin = {\"r\":0,\"l\":0,\"b\":0,\"t\":50}, font_size = 10)\n    return fig\n\nInteractive geographic scatterplots are outputted with a point for each station colored by the estimate of the yearly change in temperature during the specified month and time at that station.\n\n\nThis plot displays the estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"India\", \n                                   year_begin = 1980, \n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\nThis plot displays the estimated yearly increases in temperature during the month of September, in the interval 2000-2010, in Brazil.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"Brazil\", \n                                   year_begin = 2000, \n                                   year_end = 2010, \n                                   month = 9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThis additional SQL query function returns a dataframe of the average temperatures at stations within 0-90 degrees latitude and 0-180 degrees longitude, which represents the regions of the Northern Hemisphere (between the equator and the North Pole), during the months of the specified year of 2020.\n\ndef query_northHemi_database(year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations within\n    0-90 degrees latitude and 0-180 degrees longitude, which represents\n    the regions of the Northern Hemisphere, during a specified year.\n    \"\"\"\n    cmd =\\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    AND T.Year = {year}\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n\n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    # sort the months by order\n    df = df.sort_values(by = \"Month\")\n\n    return df\n\n\nquery_northHemi_database(2020)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\n\n\n\n\n1386\nLOP_BURI\n14.800\n100.617\nThailand\n2020\n1\n29.4\n\n\n1823\nPARTIZANSK\n43.150\n133.017\nRussia\n2020\n1\n6.4\n\n\n1824\nPASNI\n25.290\n63.344\nPakistan\n2020\n1\n25.8\n\n\n1825\nPATIALA\n30.333\n76.467\nIndia\n2020\n1\n22.8\n\n\n1826\nPATNA\n25.600\n85.100\nIndia\n2020\n1\n24.1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2119\nSHALATIN\n23.133\n35.583\nEgypt\n2020\n9\n27.0\n\n\n590\nEL_ARISH_INTL\n31.073\n33.836\nEgypt\n2020\n9\n27.0\n\n\n2044\nSALLUM_PLATEAU\n31.567\n25.133\nEgypt\n2020\n10\n19.1\n\n\n637\nFARAFRA\n27.050\n27.983\nEgypt\n2020\n11\n16.1\n\n\n595\nEL_KHOMS\n32.633\n14.300\nLibya\n2020\n12\n15.7\n\n\n\n\n2774 rows × 7 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a facet scatterplot is created representing the average temperatures of the stations in the Northern Hemisphere during 2020. This plot involves facets separated by the month.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef northHemi_plot(year, **kwargs):\n    \"\"\"\n    Returns a figure containing a scatter plot of latitude against average temperatures at \n    each station in countries within the Northern Hemisphere during a specific year. The \n    color of each bar is distinguished by temperature while the label of each bar is \n    displayed as the station, latitude, and country. Each facet is separated by month in \n    order. \n    \"\"\"\n    query_northHemi = query_northHemi_database(2020)\n    \n    fig = px.scatter(query_northHemi, \n                     x = \"LATITUDE\",\n                     y = \"Average Temperature\",\n                     title = \\\n                     f\"\"\"\n                     Average Temperatures at Stations in the Northern Hemisphere During 2020\n                     \"\"\",\n                     color = \"Average Temperature\",\n                     hover_name = \"Country\",\n                     hover_data = [\"NAME\", \"LATITUDE\", \"Country\"],\n                     width = 700,\n                     height = 2000,\n                     opacity = 0.5,\n                     color_continuous_midpoint = 0,\n                     facet_row=\"Month\",\n                     )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig_northHemi = northHemi_plot(2020)\n\nfig_northHemi.show()\n\n\n\n\nIt can be seen that much of the available data was collected in the month of January during 2020 where it is especially evident in this month that the average temperature seemed to decrease as latitude increased. This is reasonable as latitude increases as you move closer to the poles, which have a lower temperature than regions closer to the equator (0 degrees latitude). For other months, because the data is more limited, this pattern is much less clear as some months have only one data point.\n\n\n\n\n\n\nNext, building on the previous question as the Northern Hemisphere is still in focus, now I will add an additional component: seasons. Specifically, I am interested in the summer and winter seasons, and the average temperatures associated with each station in each country in the Northern Hemisphere during the summer and winter.\nThe months of summer are June, July, and August, which can be numerically expressed as 6, 7, and 8, respectively. The months of winter are December, January, and February, which can be numerically expressed as 12, 1, and 2, respectively.\n\nsummerWinterMonths = {\"Summer\": [6,7,8], \"Winter\":[12, 1, 2]}\n\nA new SQL query function is created to reorganize the data according to the summer and winter months of a specific country and year. Only the summer and winter months are of focus for this query function.\n\ndef query_summerWinter_database(season, year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations of countries \n    still within the Northern Hemisphere during the months of a specified \n    season and year.\n    \"\"\"\n    # grab each month by the values associated with the key as inputted by the parameter\n    cmd = \\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE T.year = {year}\n    AND (T.Month = {summerWinterMonths[season][0]} \n    OR T.Month = {summerWinterMonths[season][1]}\n    OR T.Month = {summerWinterMonths[season][2]})\n    AND (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    df[\"Season\"] = season\n#     df = df.sort_values(by = \"Month\")\n#     df[\"Season\"] = list(summerWinterMonths.keys())[list(summerWinterMonths.values()).index(df.get(\"Month\"))]\n#     df[\"Season\"] = df[\"Season\"].sort_values(by = \"Month\")\n\n    return df\n\nAs an example, the average temperatures during the summer season of 2012 can be seen at each station and respective countries.\n\nquery_summerWinter_database(\"Summer\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n6\n14.1\nSummer\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n6\n17.0\nSummer\n\n\n2\nABAGNAR_QI\n43.9000\n116.0000\nChina\n2012\n7\n21.3\nSummer\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n6\n19.6\nSummer\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n6\n19.8\nSummer\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3977\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n8\n18.0\nSummer\n\n\n3978\nZUNYI\n27.7000\n106.8830\nChina\n2012\n6\n25.0\nSummer\n\n\n3979\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n6\n21.1\nSummer\n\n\n3980\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n6\n16.7\nSummer\n\n\n3981\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n6\n14.0\nSummer\n\n\n\n\n3982 rows × 8 columns\n\n\n\nOn the other hand, the following displays the average temperatures during the winter season of 2012 at each station and respective countries.\n\nquery_summerWinter_database(\"Winter\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n1\n5.4\nWinter\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n1\n2.3\nWinter\n\n\n2\nABADAN\n30.3710\n48.2280\nIran\n2012\n12\n16.2\nWinter\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n1\n-21.9\nWinter\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n1\n-23.9\nWinter\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4039\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n12\n-7.6\nWinter\n\n\n4040\nZUNYI\n27.7000\n106.8830\nChina\n2012\n1\n4.5\nWinter\n\n\n4041\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n1\n-20.1\nWinter\n\n\n4042\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n1\n-2.5\nWinter\n\n\n4043\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n1\n-33.4\nWinter\n\n\n\n\n4044 rows × 8 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a bar graph is created representing the average temperatures of the stations in each country of the Northern Hemisphere during the summer and winter seasons of 2012.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef summerWinter_plot(season, year, **kwargs):\n    \"\"\"\n    Returns a figure containing a bar graph of countries against average temperatures at each station\n    during a given season and year. The color of each bar is distinguished by temperature while the \n    label of each bar is displayed as the country, station, latitude, and month. \n    \"\"\"\n    query_summerWinter = query_summerWinter_database(season, year)\n    \n    fig = px.bar(query_summerWinter, \n                            x = \"Country\",\n                            y = \"Average Temperature\",\n                            title = \\\n                            f\"\"\"\n                            Average Temperatures in °C at Each Station in the {season} of {year} for Each Country\n                            \"\"\",\n                            color = \"Average Temperature\",\n                            color_continuous_scale='Bluered_r',\n                            hover_data = [\"Country\",\"NAME\", \"LATITUDE\", \"Month\"],\n                            )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the summer of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Summer\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has the highest average temperatures at its stations among the countries in the summer of 2012. On the other hand, countries such as Singapore and Cambodia have the lowest average temperatures at their stations compared to other countries in the summer of 2012.\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the winter of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Winter\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has some of the lowest average temperatures at its stations among the countries in the winter of 2012. On the other hand, southeast Asian countries such as Laos and Myanmar have some of the highest average temperatures at their stations compared to other countries in the winter of 2012.\nFinally closing the database:\n\nconn.close()\n\n\n\n\n\nUnfortunately, one issue with the original color map gradient (color_map = px.colors.diverging.RdGy_r) is that most of the values were either very low which were hard to see if they were associated with a very light color such as yellow. This causes the majority stations with positive temperatures to be difficult to see as positive temperatures are associated with lighter color in the scale. Therefore, I changed the the color_continuous_scale to be “Bluered_r” to be more apparent.\n\n\n\nThe query functions and their corresponding figures illustrate how datasets may be manipulated in terms of the variables they depict, the different ways of integrating their tables, and the type of plot used for display. Through these changes, large datasets can be visualized in a cohesive and organized way to answer a wide variety of complex questions. Another takeaway from the resulting visualizations is that collecting larger amounts of data that involve each variable of interest is important as this can provide a better overall picture of the trends inherent in the dataset. With less points to plot, detecting these patterns become much more difficult and therefore breeds a more vague answer to research questions."
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-1-create-a-database",
    "href": "posts/bruin/HW1/index.html#part-1-create-a-database",
    "title": "HW1",
    "section": "",
    "text": "I begin by importing the necessary libaries and packages to create a database that consists of three tables: temperatures, stations, and countries. The data for these tables are extracted from csv files. The cleaning process involves some of the column names being renamed and column values adjusted accordingly. sqlite3.connect() points to the database location.\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\nfrom plotly.io import write_html\n\n\ndef prepare_df(df):\n    \"\"\"\n    Cleans the dataframe\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    \n    return(df)\n\n# creating a database in the current directory called temps.db\nwith sqlite3.connect(\"temps.db\") as conn: \n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    stations_csv = \"station-metadata.csv\"\n    stations = pd.read_csv(stations_csv)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n    countries = pd.read_csv(countries_url)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n    \n# the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\ndf.to_sql() writes to a specified table, either temperatures, stations, or countries in temps.db. Becuase the tables for the metadata in our database are from pretty small data sets, there is no need to read it in by chunks. if_exists = “replace” ensures that a new piece is added to the table each time rather than being overwritten."
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-2-write-a-query-function",
    "href": "posts/bruin/HW1/index.html#part-2-write-a-query-function",
    "title": "HW1",
    "section": "",
    "text": "Now, I will perform some basic queries on the data tables by joining them based on relational information using INNER JOIN (only rows that have common characteristics similar to the intersection of two sets).\nFor example, the id of the stations table corresponds to the id of the temperatures table.\nThe WHERE parameters take the arguments passed in to correspond to the columns of the dataframe.\nInstead of using the cursor for prototyping SQL queries, here I use pd.read_sql_query(cmd, conn) as an easier approach.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns a dataframe of the temperatures at stations specified\n    by latitude, longitude, and country within a range of months and \n    years.    \n    \"\"\"\n    # creating a database in the current directory \n    # inner join (default for pd.merge): shared columns between the two sets\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE T.year &gt;= {year_begin}\n        AND T.year &lt;= {year_end}\n        AND T.month = {month}\n        AND C.name = \"{country}\"\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.rename(columns={\"Name\" : \"Country\"})\n        \n        return df\n        # the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\n\n\n\n\n\nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nThe return value for the India example has 3152 rows. The countries “Name” column has been altered to be “Country.”"
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-3-geographic-scatter-function",
    "href": "posts/bruin/HW1/index.html#part-3-geographic-scatter-function",
    "title": "HW1",
    "section": "",
    "text": "I will now address the question:\nHow does the average yearly change in temperature vary within a given country?\n\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\n\ncoef() computes an estimate of the year-over-year average change in temperature in each month at each station using linear regression. The coefficient of Year is an estimate of the yearly change in Temp when regressing Temp against Year.\n\ndef coef(data_group):\n    \"\"\"\n    Returns the first coeffecient of the linear regression model.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nmin_obs refers to the minimum number of years of data for any given station. Because I am only interested in data of at least min_obs years worth of data in the specified month, all other data should be filtered out using df.transform() plus filtering.\nAdditional keyword arguments may also be passed to the scatter_mapbox() to control the colormap and mapbox style.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Returns a figure of the estimated yearly increase in temperature\n    of the latitude against longitude in a specified country. \n    \"\"\"\n    query_climate_df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    query_data_obs = \\\n    query_climate_df[(query_climate_df.groupby(\"NAME\")[\"Year\"].transform(len)) &gt;= min_obs]\n    \n    # first coefficient of a linear regression model at that station is computed using the .apply() method\n    coefs = query_climate_df.groupby([\"NAME\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    merge_data = pd.merge(coefs, query_climate_df, on = \"NAME\")\n    merge_data[\"Estimated Yearly Increase (°C)\"] = \\\n    round(merge_data[0], 3)\n    \n    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\",\n             6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\",\n             11:\"November\", 12:\"December\"}\n    \n    # first argument must be dataframe\n    # lat and lon arguments tell px which columns contain the latitude and longitude coordinates\n    # hover_name : what appears when hovering over the plotted points\n    # height : aspect ratio\n    fig = px.scatter_mapbox(merge_data,\n                           lat = \"LATITUDE\",\n                           lon = \"LONGITUDE\",\n                           hover_name = \"NAME\",\n                           color = \"Estimated Yearly Increase (°C)\",\n                           height = 300,\n                           opacity = .2,\n                            # colorbar is centered at 0 so that the \"middle\" of the\n                            # of the colorbar corresponds to a coefficient of 0\n                           color_continuous_midpoint = 0,\n                           labels = {\"LATITUDE\", \"LONGITUDE\"},\n                           title = \\\n                           f\"\"\"\n                           Estimates of yearly increase in temperature in {months[month]} for stations in {country}, years {year_begin} - {year_end}\n                           \"\"\",\n                           **kwargs)\n    # controls which map tiles are used in the visualization and the amount of whitespace\n    fig.update_layout(margin = {\"r\":0,\"l\":0,\"b\":0,\"t\":50}, font_size = 10)\n    return fig\n\nInteractive geographic scatterplots are outputted with a point for each station colored by the estimate of the yearly change in temperature during the specified month and time at that station.\n\n\nThis plot displays the estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"India\", \n                                   year_begin = 1980, \n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\nThis plot displays the estimated yearly increases in temperature during the month of September, in the interval 2000-2010, in Brazil.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"Brazil\", \n                                   year_begin = 2000, \n                                   year_end = 2010, \n                                   month = 9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()"
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-4-creating-two-more-sql-query-functions-corresponding-figures",
    "href": "posts/bruin/HW1/index.html#part-4-creating-two-more-sql-query-functions-corresponding-figures",
    "title": "HW1",
    "section": "",
    "text": "This additional SQL query function returns a dataframe of the average temperatures at stations within 0-90 degrees latitude and 0-180 degrees longitude, which represents the regions of the Northern Hemisphere (between the equator and the North Pole), during the months of the specified year of 2020.\n\ndef query_northHemi_database(year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations within\n    0-90 degrees latitude and 0-180 degrees longitude, which represents\n    the regions of the Northern Hemisphere, during a specified year.\n    \"\"\"\n    cmd =\\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    AND T.Year = {year}\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n\n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    # sort the months by order\n    df = df.sort_values(by = \"Month\")\n\n    return df\n\n\nquery_northHemi_database(2020)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\n\n\n\n\n1386\nLOP_BURI\n14.800\n100.617\nThailand\n2020\n1\n29.4\n\n\n1823\nPARTIZANSK\n43.150\n133.017\nRussia\n2020\n1\n6.4\n\n\n1824\nPASNI\n25.290\n63.344\nPakistan\n2020\n1\n25.8\n\n\n1825\nPATIALA\n30.333\n76.467\nIndia\n2020\n1\n22.8\n\n\n1826\nPATNA\n25.600\n85.100\nIndia\n2020\n1\n24.1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2119\nSHALATIN\n23.133\n35.583\nEgypt\n2020\n9\n27.0\n\n\n590\nEL_ARISH_INTL\n31.073\n33.836\nEgypt\n2020\n9\n27.0\n\n\n2044\nSALLUM_PLATEAU\n31.567\n25.133\nEgypt\n2020\n10\n19.1\n\n\n637\nFARAFRA\n27.050\n27.983\nEgypt\n2020\n11\n16.1\n\n\n595\nEL_KHOMS\n32.633\n14.300\nLibya\n2020\n12\n15.7\n\n\n\n\n2774 rows × 7 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a facet scatterplot is created representing the average temperatures of the stations in the Northern Hemisphere during 2020. This plot involves facets separated by the month.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef northHemi_plot(year, **kwargs):\n    \"\"\"\n    Returns a figure containing a scatter plot of latitude against average temperatures at \n    each station in countries within the Northern Hemisphere during a specific year. The \n    color of each bar is distinguished by temperature while the label of each bar is \n    displayed as the station, latitude, and country. Each facet is separated by month in \n    order. \n    \"\"\"\n    query_northHemi = query_northHemi_database(2020)\n    \n    fig = px.scatter(query_northHemi, \n                     x = \"LATITUDE\",\n                     y = \"Average Temperature\",\n                     title = \\\n                     f\"\"\"\n                     Average Temperatures at Stations in the Northern Hemisphere During 2020\n                     \"\"\",\n                     color = \"Average Temperature\",\n                     hover_name = \"Country\",\n                     hover_data = [\"NAME\", \"LATITUDE\", \"Country\"],\n                     width = 700,\n                     height = 2000,\n                     opacity = 0.5,\n                     color_continuous_midpoint = 0,\n                     facet_row=\"Month\",\n                     )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig_northHemi = northHemi_plot(2020)\n\nfig_northHemi.show()\n\n\n\n\nIt can be seen that much of the available data was collected in the month of January during 2020 where it is especially evident in this month that the average temperature seemed to decrease as latitude increased. This is reasonable as latitude increases as you move closer to the poles, which have a lower temperature than regions closer to the equator (0 degrees latitude). For other months, because the data is more limited, this pattern is much less clear as some months have only one data point.\n\n\n\n\n\n\nNext, building on the previous question as the Northern Hemisphere is still in focus, now I will add an additional component: seasons. Specifically, I am interested in the summer and winter seasons, and the average temperatures associated with each station in each country in the Northern Hemisphere during the summer and winter.\nThe months of summer are June, July, and August, which can be numerically expressed as 6, 7, and 8, respectively. The months of winter are December, January, and February, which can be numerically expressed as 12, 1, and 2, respectively.\n\nsummerWinterMonths = {\"Summer\": [6,7,8], \"Winter\":[12, 1, 2]}\n\nA new SQL query function is created to reorganize the data according to the summer and winter months of a specific country and year. Only the summer and winter months are of focus for this query function.\n\ndef query_summerWinter_database(season, year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations of countries \n    still within the Northern Hemisphere during the months of a specified \n    season and year.\n    \"\"\"\n    # grab each month by the values associated with the key as inputted by the parameter\n    cmd = \\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE T.year = {year}\n    AND (T.Month = {summerWinterMonths[season][0]} \n    OR T.Month = {summerWinterMonths[season][1]}\n    OR T.Month = {summerWinterMonths[season][2]})\n    AND (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    df[\"Season\"] = season\n#     df = df.sort_values(by = \"Month\")\n#     df[\"Season\"] = list(summerWinterMonths.keys())[list(summerWinterMonths.values()).index(df.get(\"Month\"))]\n#     df[\"Season\"] = df[\"Season\"].sort_values(by = \"Month\")\n\n    return df\n\nAs an example, the average temperatures during the summer season of 2012 can be seen at each station and respective countries.\n\nquery_summerWinter_database(\"Summer\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n6\n14.1\nSummer\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n6\n17.0\nSummer\n\n\n2\nABAGNAR_QI\n43.9000\n116.0000\nChina\n2012\n7\n21.3\nSummer\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n6\n19.6\nSummer\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n6\n19.8\nSummer\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3977\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n8\n18.0\nSummer\n\n\n3978\nZUNYI\n27.7000\n106.8830\nChina\n2012\n6\n25.0\nSummer\n\n\n3979\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n6\n21.1\nSummer\n\n\n3980\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n6\n16.7\nSummer\n\n\n3981\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n6\n14.0\nSummer\n\n\n\n\n3982 rows × 8 columns\n\n\n\nOn the other hand, the following displays the average temperatures during the winter season of 2012 at each station and respective countries.\n\nquery_summerWinter_database(\"Winter\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n1\n5.4\nWinter\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n1\n2.3\nWinter\n\n\n2\nABADAN\n30.3710\n48.2280\nIran\n2012\n12\n16.2\nWinter\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n1\n-21.9\nWinter\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n1\n-23.9\nWinter\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4039\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n12\n-7.6\nWinter\n\n\n4040\nZUNYI\n27.7000\n106.8830\nChina\n2012\n1\n4.5\nWinter\n\n\n4041\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n1\n-20.1\nWinter\n\n\n4042\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n1\n-2.5\nWinter\n\n\n4043\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n1\n-33.4\nWinter\n\n\n\n\n4044 rows × 8 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a bar graph is created representing the average temperatures of the stations in each country of the Northern Hemisphere during the summer and winter seasons of 2012.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef summerWinter_plot(season, year, **kwargs):\n    \"\"\"\n    Returns a figure containing a bar graph of countries against average temperatures at each station\n    during a given season and year. The color of each bar is distinguished by temperature while the \n    label of each bar is displayed as the country, station, latitude, and month. \n    \"\"\"\n    query_summerWinter = query_summerWinter_database(season, year)\n    \n    fig = px.bar(query_summerWinter, \n                            x = \"Country\",\n                            y = \"Average Temperature\",\n                            title = \\\n                            f\"\"\"\n                            Average Temperatures in °C at Each Station in the {season} of {year} for Each Country\n                            \"\"\",\n                            color = \"Average Temperature\",\n                            color_continuous_scale='Bluered_r',\n                            hover_data = [\"Country\",\"NAME\", \"LATITUDE\", \"Month\"],\n                            )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the summer of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Summer\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has the highest average temperatures at its stations among the countries in the summer of 2012. On the other hand, countries such as Singapore and Cambodia have the lowest average temperatures at their stations compared to other countries in the summer of 2012.\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the winter of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Winter\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has some of the lowest average temperatures at its stations among the countries in the winter of 2012. On the other hand, southeast Asian countries such as Laos and Myanmar have some of the highest average temperatures at their stations compared to other countries in the winter of 2012.\nFinally closing the database:\n\nconn.close()\n\n\n\n\n\nUnfortunately, one issue with the original color map gradient (color_map = px.colors.diverging.RdGy_r) is that most of the values were either very low which were hard to see if they were associated with a very light color such as yellow. This causes the majority stations with positive temperatures to be difficult to see as positive temperatures are associated with lighter color in the scale. Therefore, I changed the the color_continuous_scale to be “Bluered_r” to be more apparent.\n\n\n\nThe query functions and their corresponding figures illustrate how datasets may be manipulated in terms of the variables they depict, the different ways of integrating their tables, and the type of plot used for display. Through these changes, large datasets can be visualized in a cohesive and organized way to answer a wide variety of complex questions. Another takeaway from the resulting visualizations is that collecting larger amounts of data that involve each variable of interest is important as this can provide a better overall picture of the trends inherent in the dataset. With less points to plot, detecting these patterns become much more difficult and therefore breeds a more vague answer to research questions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "emilyBlog",
    "section": "",
    "text": "HW4\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW3\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\n\n\n\nweek 4\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\n\n\n\nweek 4\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nHW1\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nEmily Shi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html",
    "title": "HW2",
    "section": "",
    "text": "This activity aims to find the works of actors in a certain movie only under a specific category: “Acting.” Some actors may also hold other positions on other teams, such as a stunt double on “Crew” or an assistant director on “Production,” but this activity is only interested in the “Acting” positions of the “actors” within a specific movie.\nIn the following tutorial, I will be demonstrating the webscraping process using the TMDB sites of two movies: first from the example movie, Harry Potter and the Philosopher’s Stone, and later from my favorite movie, Kill Bill: Vol. 1.\n\n\n\n\nMy scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper.\n\n\n\n\n\nA file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\n\n\n\n\n\n\nThis is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns\n\n\n\n\n\n\n\n\n\nThis bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-1-initial-steps",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-1-initial-steps",
    "title": "HW2",
    "section": "",
    "text": "My scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper."
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "title": "HW2",
    "section": "",
    "text": "A file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "title": "HW2",
    "section": "",
    "text": "This is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#data-visualization",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#data-visualization",
    "title": "HW2",
    "section": "",
    "text": "This bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW3/index.html",
    "href": "posts/bruin/HW3/index.html",
    "title": "HW3",
    "section": "",
    "text": "To create simple web pages, we begin by installing Flask.\nFlask is a Python web framework used to create and enable web apps simply.\n\n\nTo setup flask, enter the Anaconda Prompt terminal window and type in conda activate PIC16B-24W and pip install flask. Then, set the Flask environment to “development” through set FLASK_ENV=development. This allows the developer to debug by viewing the specific error messages through the terminal. When the web page is ready to be viewed, simply cd to the correct directory location and type in flask run.\n\n\n\nWithin a newly created templates\\ folder, we will create three HTML files that will be the layout for our three web pages: base.html, submit.html, and view.html.\nbase.html acts as the home page that the user first lands on upon visiting the link. It contains links to the following web pages, submit.html and view.html, that serve other functions. It also contains a content section where the messages will later appear in. The styling of the page is due to the import of style.css which adds color and changes the font.\n\n\n\n# header for HTML files\n&lt;!doctype html&gt;\n# link to a stylesheet (style.css) in the 'static' directory\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n# main title for base.html\n&lt;title&gt;{% block title %}{% endblock %} - Home Page &lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Message Center&lt;/h1&gt;\n  &lt;ul&gt;\n    # renders as a link to either submit or view messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nsubmit.html is the page where the user visits to submit a message. It extends base.html in that it has the same styling and content section where messages appear. Here, there are labels and input boxes that allow the user to input a message and handle (username). Lastly, there is a button to submit the form, which sends the information to the data table within the database to then be viewed in view.html.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit a Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n    &lt;label for=\"message\"&gt;Please write a message:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;\n    &lt;label for=\"handle\"&gt;What is your handle or name?&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;\n    &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n\n{% endblock %}\n\nview.html is the page where the user visits to view submitted messages. It extends base.html in that it has the same styling and content section where messages appear. Here, within the “content” block, a for loop retrieves the information in the database to then display the handle and corresponding message in a dictionary-like format.\nIt takes advantage of the fact that Jinja tags support indexing of objects, where message[0] contains the handle and message[1] contains the message.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block title %}\nView Messages\n{% endblock %}\n\n{% block content %}\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n# rand_mess passed in by view() function in app.py\n{% for message in rand_mess %}\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\n\n\n\n\n\n\nget_message_db() creates the database and checks if there is a database called message_db in the g attribute of the app through the try block. If that is not found, within the except block, a connection that is an attribute of g to the database must be made. Additionally, the table messages needs to be checked if it exists in the database or not. If not, create it using the SQL command CREATE TABLE IF NOT EXISTS and ensure that there is both a handle and message column (both being of text type).\nLastly, this function returns the g.message_db connection. import sqlite3 is necessary for the creation of the database.\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                              handle TEXT,\n                              message TEXT)''')\n        g.message_db.commit()\n        return g.message_db\n\n\n\n\ninsert_message() extracts the message and handle from request. Then, it assigns the inputted message and handle to variables to then be inserted into the datatable under their corresponding columns. Lastly, the database connnection is closed.\n\ndef insert_message(request):\n    message = request.form['message']\n    handle = request.form['handle']\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # SQL query to insert the message into the 'messages' table\n    insert_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    \n    cursor = db.cursor()\n    \n    # Execute the SQL query with the handle and message as parameters\n    cursor.execute(insert_query, (handle, message))\n    \n    # Commit the changes to the database\n    db.commit()\n    db.close()\n\n\n\n\nsubmit allows the submitted information to be entered into the database. When a submission has been made, the page reloads and allows for a new submission to be made with empty entry fields.\nThe @app.route() line is necessary for directing the webpage to that particular rendered html page within the url. In this case, the main url plus /submit at the end will land the user on the rendered submit.html page.\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    # POST used to send data to a server to create/update\n    if request.method == 'POST':\n        # this function handles inserting a user message into the db\n        insert_message(request)\n        # render submit.html \n        return render_template('submit.html')\n    else:\n        # if nothing is posted, just render the same page again\n        return render_template('submit.html')\n\n\n\n\nrandom_messages(n) takes in a number of messages as a parameter. It opens a database and selects the handle and corresponding message by random and returns that information as a variable.\n\ndef random_messages(n):\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # Create a cursor object to execute SQL commands\n    cursor = db.cursor()    \n    \n    # SQL query to insert the message into the 'messages' table\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))\n    messages = cursor.fetchall()\n   \n    # Commit the changes to the database\n    db.commit()\n    \n    # close the db\n    db.close()\n    \n    return messages\n\n\n\n\nview calls random_messages with a parameter of five, which would be the number of random messages in the collection returned. It then sends the returned information, being the messages and handles, as a variable to view.html to then be viewed.\n\n@app.route('/view')\ndef view():\n    # grabs 5 (or less) random messages \n    rand_mess = random_messages(5)\n    # passes the messages as an argument to render_template()\n    return render_template('view.html', rand_mess = rand_mess)\n\n\n\n\n\nLastly, to add some style to our webpage, we will change the font and incorporate color. The font is changed to “Courier New” of the “Lucida Console” font-family, while the overall background color is changed to #CD5C5C which resembles a pretty pinkish-red. The background color of the text is changed to #FFFFF0, which is a creamy white color.\n\nbody {\n    font-family: \"Lucida Console\", \"Courier New\", monospace;\n    background-color: #CD5C5C;\n}\nh1 {\n    color: #FFFFF0;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #FFFFF0;\n    padding: 10px;\n    margin-bottom: 5px;\n}\n\n\n\n\nUpon landing on the url, the first page is the rendered base.html. Here, you see the links to submit and view messages.\n\n\n\nmainpage.png\n\n\nAfter clicking the link to submit a message, you land on the rendered submit.html where you are able to fill in the forms to write a message and give a corresponding name.\n\n\n\nsubmitmessage.png\n\n\n\n\n\nwrittenmessage.png\n\n\nAfter submitting a few messages, clicking on the link to view messages takes you to the rendered view.html. Here, you can view the submitted messages (in random order).\n\n\n\nviewmessages.png\n\n\nThat’s all! Hope you enjoyed this short tutorial!\n\n\nhttps://github.com/emilyrshi/PIC16B_HW3"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#flask-setup",
    "href": "posts/bruin/HW3/index.html#flask-setup",
    "title": "HW3",
    "section": "",
    "text": "To setup flask, enter the Anaconda Prompt terminal window and type in conda activate PIC16B-24W and pip install flask. Then, set the Flask environment to “development” through set FLASK_ENV=development. This allows the developer to debug by viewing the specific error messages through the terminal. When the web page is ready to be viewed, simply cd to the correct directory location and type in flask run."
  },
  {
    "objectID": "posts/bruin/HW3/index.html#writing-the-templates-files-html",
    "href": "posts/bruin/HW3/index.html#writing-the-templates-files-html",
    "title": "HW3",
    "section": "",
    "text": "Within a newly created templates\\ folder, we will create three HTML files that will be the layout for our three web pages: base.html, submit.html, and view.html.\nbase.html acts as the home page that the user first lands on upon visiting the link. It contains links to the following web pages, submit.html and view.html, that serve other functions. It also contains a content section where the messages will later appear in. The styling of the page is due to the import of style.css which adds color and changes the font.\n\n\n\n# header for HTML files\n&lt;!doctype html&gt;\n# link to a stylesheet (style.css) in the 'static' directory\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n# main title for base.html\n&lt;title&gt;{% block title %}{% endblock %} - Home Page &lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Message Center&lt;/h1&gt;\n  &lt;ul&gt;\n    # renders as a link to either submit or view messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nsubmit.html is the page where the user visits to submit a message. It extends base.html in that it has the same styling and content section where messages appear. Here, there are labels and input boxes that allow the user to input a message and handle (username). Lastly, there is a button to submit the form, which sends the information to the data table within the database to then be viewed in view.html.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit a Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n    &lt;label for=\"message\"&gt;Please write a message:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;\n    &lt;label for=\"handle\"&gt;What is your handle or name?&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;\n    &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n\n{% endblock %}\n\nview.html is the page where the user visits to view submitted messages. It extends base.html in that it has the same styling and content section where messages appear. Here, within the “content” block, a for loop retrieves the information in the database to then display the handle and corresponding message in a dictionary-like format.\nIt takes advantage of the fact that Jinja tags support indexing of objects, where message[0] contains the handle and message[1] contains the message.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block title %}\nView Messages\n{% endblock %}\n\n{% block content %}\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n# rand_mess passed in by view() function in app.py\n{% for message in rand_mess %}\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#the-five-functions-of-app.py",
    "href": "posts/bruin/HW3/index.html#the-five-functions-of-app.py",
    "title": "HW3",
    "section": "",
    "text": "get_message_db() creates the database and checks if there is a database called message_db in the g attribute of the app through the try block. If that is not found, within the except block, a connection that is an attribute of g to the database must be made. Additionally, the table messages needs to be checked if it exists in the database or not. If not, create it using the SQL command CREATE TABLE IF NOT EXISTS and ensure that there is both a handle and message column (both being of text type).\nLastly, this function returns the g.message_db connection. import sqlite3 is necessary for the creation of the database.\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                              handle TEXT,\n                              message TEXT)''')\n        g.message_db.commit()\n        return g.message_db\n\n\n\n\ninsert_message() extracts the message and handle from request. Then, it assigns the inputted message and handle to variables to then be inserted into the datatable under their corresponding columns. Lastly, the database connnection is closed.\n\ndef insert_message(request):\n    message = request.form['message']\n    handle = request.form['handle']\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # SQL query to insert the message into the 'messages' table\n    insert_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    \n    cursor = db.cursor()\n    \n    # Execute the SQL query with the handle and message as parameters\n    cursor.execute(insert_query, (handle, message))\n    \n    # Commit the changes to the database\n    db.commit()\n    db.close()\n\n\n\n\nsubmit allows the submitted information to be entered into the database. When a submission has been made, the page reloads and allows for a new submission to be made with empty entry fields.\nThe @app.route() line is necessary for directing the webpage to that particular rendered html page within the url. In this case, the main url plus /submit at the end will land the user on the rendered submit.html page.\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    # POST used to send data to a server to create/update\n    if request.method == 'POST':\n        # this function handles inserting a user message into the db\n        insert_message(request)\n        # render submit.html \n        return render_template('submit.html')\n    else:\n        # if nothing is posted, just render the same page again\n        return render_template('submit.html')\n\n\n\n\nrandom_messages(n) takes in a number of messages as a parameter. It opens a database and selects the handle and corresponding message by random and returns that information as a variable.\n\ndef random_messages(n):\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # Create a cursor object to execute SQL commands\n    cursor = db.cursor()    \n    \n    # SQL query to insert the message into the 'messages' table\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))\n    messages = cursor.fetchall()\n   \n    # Commit the changes to the database\n    db.commit()\n    \n    # close the db\n    db.close()\n    \n    return messages\n\n\n\n\nview calls random_messages with a parameter of five, which would be the number of random messages in the collection returned. It then sends the returned information, being the messages and handles, as a variable to view.html to then be viewed.\n\n@app.route('/view')\ndef view():\n    # grabs 5 (or less) random messages \n    rand_mess = random_messages(5)\n    # passes the messages as an argument to render_template()\n    return render_template('view.html', rand_mess = rand_mess)"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#css-styling",
    "href": "posts/bruin/HW3/index.html#css-styling",
    "title": "HW3",
    "section": "",
    "text": "Lastly, to add some style to our webpage, we will change the font and incorporate color. The font is changed to “Courier New” of the “Lucida Console” font-family, while the overall background color is changed to #CD5C5C which resembles a pretty pinkish-red. The background color of the text is changed to #FFFFF0, which is a creamy white color.\n\nbody {\n    font-family: \"Lucida Console\", \"Courier New\", monospace;\n    background-color: #CD5C5C;\n}\nh1 {\n    color: #FFFFF0;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #FFFFF0;\n    padding: 10px;\n    margin-bottom: 5px;\n}"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#my-webpage-in-action",
    "href": "posts/bruin/HW3/index.html#my-webpage-in-action",
    "title": "HW3",
    "section": "",
    "text": "Upon landing on the url, the first page is the rendered base.html. Here, you see the links to submit and view messages.\n\n\n\nmainpage.png\n\n\nAfter clicking the link to submit a message, you land on the rendered submit.html where you are able to fill in the forms to write a message and give a corresponding name.\n\n\n\nsubmitmessage.png\n\n\n\n\n\nwrittenmessage.png\n\n\nAfter submitting a few messages, clicking on the link to view messages takes you to the rendered view.html. Here, you can view the submitted messages (in random order).\n\n\n\nviewmessages.png\n\n\nThat’s all! Hope you enjoyed this short tutorial!\n\n\nhttps://github.com/emilyrshi/PIC16B_HW3"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]