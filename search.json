[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html",
    "href": "posts/bruin/HW5_resubmit/index.html",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "In this tutorial, we will look at how different machine learning algorithms uses image classification to distinguish between pictures of dogs and cats.\nWe will feed data through Tensorflow Datasets and classify images in Keras. Tensorflow Datasets allow us to organize operations on training, validation, and test datasets. Additionally, manipulating and augmenting our datasets allow each of the models to adopt and learn patterns more efficiently. Lastly, we will use transfer learning by incorporating pre-trained models to perform new tasks for us, which simplifies the process.\nTo run the models in this tutorial, we will enable a GPU runtime to speed the processes.\nWe begin by installing the latest version of keras in addition to importing the necessary packages.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 6.6 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\n\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\n\n!nvidia-smi\n\nMon Mar 11 04:12:05 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   43C    P0              26W /  70W |    105MiB / 15360MiB |      3%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nThe following creates the datasets for training, validation, and testing. The dataset is the pipeline that feeds data to a machine learning model. Datasets are used when it is not necessarily practical to load all data into memory.\n\n# all of the following provided in the instructions\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nThe dataset contains images of different sizes, so we will resize them to a fixed size of 150x150 pixels.\n\n# all of the following provided in the instructions\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nHere, we will rapidly read the data.\nbatch_size determines how many data points are gathered from the directory at once.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\n\nTo visualize our dataset, we will write a function that creates a two-row visualization. The first row of the plot contains three random pictures of cats while the second row contains three random pictures of dogs.\n\nimport random\nimport tensorflow as tf\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\n\n\ndef two_row_vis(dataset, num_rows, num_cols, num_samples):\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # take method gets a piece of the dataset, specifically one batch (or 32\n    # images with labels) from the training data\n    # skip takes the element at the nth value (being num_samples, which is\n    # a random integer between 1 and 10) away\n    for i, (images, labels) in enumerate(dataset.skip(num_samples).take(1)):\n        cat_count = 0\n        dog_count = 0\n        for image, label in zip(images, labels):\n            # row of cats\n            if label == 0 and cat_count &lt; num_cols:\n                axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[0, cat_count].set_title('Cat')\n                axes[0, cat_count].axis(\"off\")\n                cat_count += 1\n                # row of dogs\n            elif label == 1 and dog_count &lt; num_cols:\n                axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[1, dog_count].set_title('Dog')\n                axes[1, dog_count].axis(\"off\")\n                dog_count += 1\n            if cat_count == num_cols and dog_count == num_cols:\n                break\n\n    plt.show()\n\n\ntwo_row_vis(train_ds, 2, 3, random.randint(1,10))\n\n\n\n\n\n\n\n\n\n\n\nNow, we will create an iterator called labels_iterator.\n\n# provided in instructions\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nUsing labels_iterator, we will compute the number of images in the training data with label 0 (which represents the cats) and label 1 (which represents the dogs).\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (Cat):\", cat_count)\nprint(\"Number of images with label 1 (Dog):\", dog_count)\n\nNumber of images with label 0 (Cat): 4637\nNumber of images with label 1 (Dog): 4668\n\n\nAs we can see, there are more images of dogs than cats in this dataset.\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the most frequent label is label 1 which is 4668, the baseline model would be 4668 out of the total number of images, or (4668/(4637+4668))%, or 50.2% accuracy.\n\n\n\nNow, we create a keras.Sequential model using some of the layers discussed in class: two Conv2D layers, two MaxPooling2D layers, one Flatten layer, two Dense layer, and one Dropout layer.\n\n# From \"Keras 2\" lecture colab\n\nfrom keras import datasets, layers, models\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nTo train a model on a dataset:\n\n# provided in instructions\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 23s 125ms/step - accuracy: 0.4789 - loss: 54.2556 - val_accuracy: 0.5804 - val_loss: 0.9293\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.5670 - loss: 1.4733 - val_accuracy: 0.5868 - val_loss: 0.7373\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5776 - loss: 1.4252 - val_accuracy: 0.5890 - val_loss: 0.7001\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6015 - loss: 1.3344 - val_accuracy: 0.5959 - val_loss: 0.8494\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.6089 - loss: 1.3221 - val_accuracy: 0.5963 - val_loss: 0.7441\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 39ms/step - accuracy: 0.6569 - loss: 1.2101 - val_accuracy: 0.5623 - val_loss: 0.8668\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6899 - loss: 1.1611 - val_accuracy: 0.5959 - val_loss: 0.9356\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.7239 - loss: 1.0987 - val_accuracy: 0.5894 - val_loss: 1.0127\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7462 - loss: 1.0765 - val_accuracy: 0.6015 - val_loss: 1.1886\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.7754 - loss: 1.0152 - val_accuracy: 0.5924 - val_loss: 1.2770\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7824 - loss: 0.9859 - val_accuracy: 0.5933 - val_loss: 1.3091\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8026 - loss: 0.9813 - val_accuracy: 0.5916 - val_loss: 1.2550\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 33ms/step - accuracy: 0.8126 - loss: 0.9585 - val_accuracy: 0.6019 - val_loss: 1.5850\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8314 - loss: 0.9304 - val_accuracy: 0.6079 - val_loss: 1.8018\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8459 - loss: 0.8883 - val_accuracy: 0.5903 - val_loss: 1.9248\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8485 - loss: 0.9014 - val_accuracy: 0.5825 - val_loss: 2.1410\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8452 - loss: 0.9139 - val_accuracy: 0.6036 - val_loss: 2.2382\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8425 - loss: 0.9162 - val_accuracy: 0.5778 - val_loss: 2.5244\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8409 - loss: 0.9330 - val_accuracy: 0.5804 - val_loss: 2.1509\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8483 - loss: 0.9088 - val_accuracy: 0.5817 - val_loss: 2.5131\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 57.8% and 60.8% during training. Evidently, this model is able to consistently achieve at least 55% validation accuracy.\nCompared to the baseline of 50.2%, this model performed at least 7% better.\nOverfitting occurs when the training accuracy is much higher than the validation accuracy. In model1, overfitting does occur as the training accuracy may be at least ten percent higher than the validation accuracy, especially for later epochs.\n\n\n\n\nThis next model will involve data augmentation layers. Data augmentation refers to the practice of including modified copies of the same image in the training set.\nFor example, flipped or rotated images are still the same image in the content they contain. Therefore, incorporating transformed versions of the image in the training process help the model learn these invariant features of the input images.\nWe begin by creating a keras.layers.RandomFlip() and keras.layers.RandomRotation() layer.\n\nfor images, labels in train_ds.take(1):\n    # Assuming you want the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n\n# Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\n\n\nThen, we create a plot of the original image and two more copies of that image where RandomFlip() has been applied.\n\ndata_augmentation_flipped = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n\n\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\nfor i in range(2):\n    augmented_image = data_augmentation_flipped(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThen, we create a plot of the original image and two more copies of that image where RandomRotation() has been applied.\n\ndata_augmentation_rotated = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n])\n\n\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\nfor i in range(2):\n    augmented_image = data_augmentation_rotated(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel2.summary()\n\nModel: \"sequential_8\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_7 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_2 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_9 (Conv2D)                    │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_6 (MaxPooling2D)       │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_10 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_7 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_11 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_3 (Flatten)                  │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (Dense)                      │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTo train a model on a dataset:\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.4742 - loss: 52.9159 - val_accuracy: 0.5868 - val_loss: 0.6810\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 225ms/step - accuracy: 0.5281 - loss: 1.4451 - val_accuracy: 0.6234 - val_loss: 0.6575\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 224ms/step - accuracy: 0.5251 - loss: 1.4545 - val_accuracy: 0.6432 - val_loss: 0.6396\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 231ms/step - accuracy: 0.5415 - loss: 1.4153 - val_accuracy: 0.6582 - val_loss: 0.6400\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.5409 - loss: 1.4078 - val_accuracy: 0.6543 - val_loss: 0.6211\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.5449 - loss: 1.3784 - val_accuracy: 0.6793 - val_loss: 0.5936\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5538 - loss: 1.3775 - val_accuracy: 0.6849 - val_loss: 0.5985\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5570 - loss: 1.3586 - val_accuracy: 0.6651 - val_loss: 0.6062\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5465 - loss: 1.3940 - val_accuracy: 0.6935 - val_loss: 0.5869\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 230ms/step - accuracy: 0.5583 - loss: 1.3649 - val_accuracy: 0.6776 - val_loss: 0.6089\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5554 - loss: 1.3624 - val_accuracy: 0.6999 - val_loss: 0.6217\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5609 - loss: 1.3854 - val_accuracy: 0.7077 - val_loss: 0.5896\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5592 - loss: 1.3439 - val_accuracy: 0.6952 - val_loss: 0.6142\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5713 - loss: 1.3408 - val_accuracy: 0.6887 - val_loss: 0.5935\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.5612 - loss: 1.3488 - val_accuracy: 0.6862 - val_loss: 0.5874\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5692 - loss: 1.3433 - val_accuracy: 0.7059 - val_loss: 0.5640\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5623 - loss: 1.3483 - val_accuracy: 0.7102 - val_loss: 0.5722\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5697 - loss: 1.3539 - val_accuracy: 0.6797 - val_loss: 0.5914\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 230ms/step - accuracy: 0.5318 - loss: 1.5118 - val_accuracy: 0.6109 - val_loss: 0.7261\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5268 - loss: 1.4176 - val_accuracy: 0.6303 - val_loss: 0.6708\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 58.7% and 71% during training. Evidently, this model is able to consistently achieve at least 60% validation accuracy. Scores of 70% and above were also achieved.\nCompared to the validation accuracy of model1, model2 performed roughly the same as the lowest and highest accuracies were about 59-60% and 70-71%, respectively.\nIn model2, overfitting does not occur as the training accuracies are consistently lower than the validation accuracies.\n\n\n\n\n\nThis next model will involve a preprocessing layer. This makes simple transformations to the input data.\nFor example, it may be the case that the original input image has pixels with RGB values between 0 and 255, but some models train faster with RGB values normalized between 0 and 1 (grayscale), or possibly between -1 and 1. Scaling the weights still result in mathematically identical situations. However, if the scaling process was handled prior to the training process, more of the training energy can be spent on the actual signal in the data rather than on the adjusting of the weights to the data scale.\nFor example, flipped or rotated images are still the same image in the content they contain. Therefore, incorporating transformed versions of the image in the training process help the model learn these invariant features of the input images.\n\n# all of the following found in the instructions\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = models.Sequential([\n    preprocessor,\n    # layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    # layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.RandomFlip(),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2), # number of classes\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel3.summary()\n\nModel: \"sequential_11\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_15 (Functional)           │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_10 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_5 (RandomRotation)   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_18 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_12 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_19 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_13 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_20 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_6 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (Dropout)                  │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (Dense)                     │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTo train a model on a dataset:\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 223ms/step - accuracy: 0.7916 - loss: 0.4508 - val_accuracy: 0.7752 - val_loss: 0.4804\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7988 - loss: 0.4413 - val_accuracy: 0.7876 - val_loss: 0.4608\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 219ms/step - accuracy: 0.7954 - loss: 0.4402 - val_accuracy: 0.7915 - val_loss: 0.4551\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7963 - loss: 0.4391 - val_accuracy: 0.7932 - val_loss: 0.4609\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 234ms/step - accuracy: 0.7995 - loss: 0.4307 - val_accuracy: 0.7863 - val_loss: 0.4774\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.8069 - loss: 0.4216 - val_accuracy: 0.7756 - val_loss: 0.4769\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8025 - loss: 0.4239 - val_accuracy: 0.8061 - val_loss: 0.4425\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8117 - loss: 0.4163 - val_accuracy: 0.8027 - val_loss: 0.4322\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8095 - loss: 0.4162 - val_accuracy: 0.8048 - val_loss: 0.4488\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.8090 - loss: 0.4102 - val_accuracy: 0.8117 - val_loss: 0.4229\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8155 - loss: 0.4056 - val_accuracy: 0.8095 - val_loss: 0.4274\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.8182 - loss: 0.4026 - val_accuracy: 0.8087 - val_loss: 0.4480\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.8155 - loss: 0.3973 - val_accuracy: 0.8104 - val_loss: 0.4418\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 234ms/step - accuracy: 0.8141 - loss: 0.3956 - val_accuracy: 0.8044 - val_loss: 0.4529\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.8310 - loss: 0.3849 - val_accuracy: 0.8087 - val_loss: 0.4594\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8221 - loss: 0.3941 - val_accuracy: 0.8052 - val_loss: 0.4294\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 231ms/step - accuracy: 0.8214 - loss: 0.3831 - val_accuracy: 0.8005 - val_loss: 0.4657\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8235 - loss: 0.3840 - val_accuracy: 0.8078 - val_loss: 0.4360\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.8274 - loss: 0.3761 - val_accuracy: 0.8229 - val_loss: 0.4012\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8344 - loss: 0.3734 - val_accuracy: 0.8113 - val_loss: 0.4393\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 77.5% and 82.3% during training. Evidently, this model is able to consistently achieve at least 80% validation accuracy.\nCompared to the validation accuracy of model1, model3 performed at least 18% better, with the lowest accuracies performing roughly 19% better and the highest accuracies performing roughly 21% better.\nIn model3, overfitting does not occur as the training accuracies are overall lower than the validation accuracies.\n\n\n\n\nFor this last model, we will be using a pre-existing model for the task of distinguishing between cats and dogs. Unlike the previous models, this one will not be trained from scratch. Instead, we will incorporate a pre-existing “base model” into the full model, which is ultimately what will be trained.\nThe benefit of using a pre-existing model is that someone might already have trained a model that performs a similar task, and this model could have already learned relevant patterns. This should lead us to seeing increased validation accuracy.\nThe following downloads MobileNetV3Large and configures it aas a layer to be included in this model.\n\n# all of the following provided in instructions\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v3.py:512: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n\nmodel4 will make use of MobileNetV3Large through incorporating base_model and IMG_SHAPE.\n\nmodel4 = models.Sequential([\n    # following two are the data augmentation layers from Part 3\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    base_model_layer, # constructed above\n    layers.GlobalMaxPool2D(), # additional layer in between Layer 3 and 4\n    layers.Dense(2) # Layer 4: Dense(2) layer to perform the classification\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel4.compile(optimizer = 'adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n              metrics = ['accuracy'])\n\n\nmodel4.summary()\n\nModel: \"sequential_12\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_11 (RandomFlip)          │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_6 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_18 (Functional)           │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (Dense)                     │ (None, 2)                   │           1,922 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nClearly, there is a lot of complexity hidden in the base_model_layer. There are 1,922 parameters that are trainable out of almost three million total parameters, which is a significantly smaller ratio of trainable parameters compared to the other models.\nTo train a model on a dataset:\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 120s 661ms/step - accuracy: 0.7845 - loss: 1.8376 - val_accuracy: 0.9583 - val_loss: 0.3296\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 412ms/step - accuracy: 0.9025 - loss: 0.6569 - val_accuracy: 0.9570 - val_loss: 0.2781\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 419ms/step - accuracy: 0.9078 - loss: 0.5825 - val_accuracy: 0.9665 - val_loss: 0.2192\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 634ms/step - accuracy: 0.9132 - loss: 0.4687 - val_accuracy: 0.9660 - val_loss: 0.1860\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 632ms/step - accuracy: 0.9208 - loss: 0.3738 - val_accuracy: 0.9471 - val_loss: 0.2629\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 629ms/step - accuracy: 0.9141 - loss: 0.4168 - val_accuracy: 0.9574 - val_loss: 0.1658\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 633ms/step - accuracy: 0.9240 - loss: 0.3267 - val_accuracy: 0.9592 - val_loss: 0.1789\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9204 - loss: 0.3412 - val_accuracy: 0.9355 - val_loss: 0.3163\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9210 - loss: 0.3376 - val_accuracy: 0.9536 - val_loss: 0.2004\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9243 - loss: 0.2820 - val_accuracy: 0.9454 - val_loss: 0.2445\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 59s 407ms/step - accuracy: 0.9164 - loss: 0.3396 - val_accuracy: 0.9420 - val_loss: 0.2534\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 630ms/step - accuracy: 0.9270 - loss: 0.2851 - val_accuracy: 0.9549 - val_loss: 0.1886\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 408ms/step - accuracy: 0.9106 - loss: 0.3796 - val_accuracy: 0.9561 - val_loss: 0.1964\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 416ms/step - accuracy: 0.9262 - loss: 0.2683 - val_accuracy: 0.9613 - val_loss: 0.1323\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 421ms/step - accuracy: 0.9236 - loss: 0.2740 - val_accuracy: 0.9364 - val_loss: 0.2902\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 415ms/step - accuracy: 0.9214 - loss: 0.2795 - val_accuracy: 0.9544 - val_loss: 0.1839\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 416ms/step - accuracy: 0.9293 - loss: 0.2429 - val_accuracy: 0.9540 - val_loss: 0.1685\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 414ms/step - accuracy: 0.9177 - loss: 0.2666 - val_accuracy: 0.9445 - val_loss: 0.2345\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 413ms/step - accuracy: 0.9192 - loss: 0.3048 - val_accuracy: 0.9506 - val_loss: 0.2221\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 633ms/step - accuracy: 0.9274 - loss: 0.2557 - val_accuracy: 0.9321 - val_loss: 0.3122\n\n\nVisualize the training history:\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 93.2% and 96.7% during training. Evidently, this model is able to consistently achieve at least 93% validation accuracy.\nCompared to the validation accuracy of model1, model4 performed at least 35% better, with the lowest and highest accuracies performing roughly 35% better.\nIn model4, overfitting does not occur as the training accuracies are consistently lower than the validation accuracies.\n\n\n\n\nThe most performant model is model4which consistently achieves at elast 93% validation accuracy!\nNow, we will evaluate the accuracy of this model on the unseen test_ds.\n\nmodel4.evaluate(test_ds)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 11s 288ms/step - accuracy: 0.9756 - loss: 0.0746\n\n\n[0.07890091091394424, 0.9733448028564453]\n\n\nBy evaluating model4 on the unseen test_ds, it can be seen that this model performs quite well at around 98% accuracy!"
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#part-1-loading-packages-and-obtaining-data",
    "href": "posts/bruin/HW5_resubmit/index.html#part-1-loading-packages-and-obtaining-data",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "In this tutorial, we will look at how different machine learning algorithms uses image classification to distinguish between pictures of dogs and cats.\nWe will feed data through Tensorflow Datasets and classify images in Keras. Tensorflow Datasets allow us to organize operations on training, validation, and test datasets. Additionally, manipulating and augmenting our datasets allow each of the models to adopt and learn patterns more efficiently. Lastly, we will use transfer learning by incorporating pre-trained models to perform new tasks for us, which simplifies the process.\nTo run the models in this tutorial, we will enable a GPU runtime to speed the processes.\nWe begin by installing the latest version of keras in addition to importing the necessary packages.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 6.6 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\n\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\n\n!nvidia-smi\n\nMon Mar 11 04:12:05 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   43C    P0              26W /  70W |    105MiB / 15360MiB |      3%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nThe following creates the datasets for training, validation, and testing. The dataset is the pipeline that feeds data to a machine learning model. Datasets are used when it is not necessarily practical to load all data into memory.\n\n# all of the following provided in the instructions\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nThe dataset contains images of different sizes, so we will resize them to a fixed size of 150x150 pixels.\n\n# all of the following provided in the instructions\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nHere, we will rapidly read the data.\nbatch_size determines how many data points are gathered from the directory at once.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#working-with-datasets",
    "href": "posts/bruin/HW5_resubmit/index.html#working-with-datasets",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "To visualize our dataset, we will write a function that creates a two-row visualization. The first row of the plot contains three random pictures of cats while the second row contains three random pictures of dogs.\n\nimport random\nimport tensorflow as tf\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\n\n\ndef two_row_vis(dataset, num_rows, num_cols, num_samples):\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # take method gets a piece of the dataset, specifically one batch (or 32\n    # images with labels) from the training data\n    # skip takes the element at the nth value (being num_samples, which is\n    # a random integer between 1 and 10) away\n    for i, (images, labels) in enumerate(dataset.skip(num_samples).take(1)):\n        cat_count = 0\n        dog_count = 0\n        for image, label in zip(images, labels):\n            # row of cats\n            if label == 0 and cat_count &lt; num_cols:\n                axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[0, cat_count].set_title('Cat')\n                axes[0, cat_count].axis(\"off\")\n                cat_count += 1\n                # row of dogs\n            elif label == 1 and dog_count &lt; num_cols:\n                axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[1, dog_count].set_title('Dog')\n                axes[1, dog_count].axis(\"off\")\n                dog_count += 1\n            if cat_count == num_cols and dog_count == num_cols:\n                break\n\n    plt.show()\n\n\ntwo_row_vis(train_ds, 2, 3, random.randint(1,10))"
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#check-label-frequencies",
    "href": "posts/bruin/HW5_resubmit/index.html#check-label-frequencies",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "Now, we will create an iterator called labels_iterator.\n\n# provided in instructions\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nUsing labels_iterator, we will compute the number of images in the training data with label 0 (which represents the cats) and label 1 (which represents the dogs).\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (Cat):\", cat_count)\nprint(\"Number of images with label 1 (Dog):\", dog_count)\n\nNumber of images with label 0 (Cat): 4637\nNumber of images with label 1 (Dog): 4668\n\n\nAs we can see, there are more images of dogs than cats in this dataset.\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the most frequent label is label 1 which is 4668, the baseline model would be 4668 out of the total number of images, or (4668/(4637+4668))%, or 50.2% accuracy."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#model-1-basic-sequential-model",
    "href": "posts/bruin/HW5_resubmit/index.html#model-1-basic-sequential-model",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "Now, we create a keras.Sequential model using some of the layers discussed in class: two Conv2D layers, two MaxPooling2D layers, one Flatten layer, two Dense layer, and one Dropout layer.\n\n# From \"Keras 2\" lecture colab\n\nfrom keras import datasets, layers, models\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nTo train a model on a dataset:\n\n# provided in instructions\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 23s 125ms/step - accuracy: 0.4789 - loss: 54.2556 - val_accuracy: 0.5804 - val_loss: 0.9293\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.5670 - loss: 1.4733 - val_accuracy: 0.5868 - val_loss: 0.7373\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5776 - loss: 1.4252 - val_accuracy: 0.5890 - val_loss: 0.7001\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6015 - loss: 1.3344 - val_accuracy: 0.5959 - val_loss: 0.8494\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.6089 - loss: 1.3221 - val_accuracy: 0.5963 - val_loss: 0.7441\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 39ms/step - accuracy: 0.6569 - loss: 1.2101 - val_accuracy: 0.5623 - val_loss: 0.8668\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6899 - loss: 1.1611 - val_accuracy: 0.5959 - val_loss: 0.9356\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.7239 - loss: 1.0987 - val_accuracy: 0.5894 - val_loss: 1.0127\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7462 - loss: 1.0765 - val_accuracy: 0.6015 - val_loss: 1.1886\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.7754 - loss: 1.0152 - val_accuracy: 0.5924 - val_loss: 1.2770\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7824 - loss: 0.9859 - val_accuracy: 0.5933 - val_loss: 1.3091\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8026 - loss: 0.9813 - val_accuracy: 0.5916 - val_loss: 1.2550\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 33ms/step - accuracy: 0.8126 - loss: 0.9585 - val_accuracy: 0.6019 - val_loss: 1.5850\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8314 - loss: 0.9304 - val_accuracy: 0.6079 - val_loss: 1.8018\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8459 - loss: 0.8883 - val_accuracy: 0.5903 - val_loss: 1.9248\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8485 - loss: 0.9014 - val_accuracy: 0.5825 - val_loss: 2.1410\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8452 - loss: 0.9139 - val_accuracy: 0.6036 - val_loss: 2.2382\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8425 - loss: 0.9162 - val_accuracy: 0.5778 - val_loss: 2.5244\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8409 - loss: 0.9330 - val_accuracy: 0.5804 - val_loss: 2.1509\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8483 - loss: 0.9088 - val_accuracy: 0.5817 - val_loss: 2.5131\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 57.8% and 60.8% during training. Evidently, this model is able to consistently achieve at least 55% validation accuracy.\nCompared to the baseline of 50.2%, this model performed at least 7% better.\nOverfitting occurs when the training accuracy is much higher than the validation accuracy. In model1, overfitting does occur as the training accuracy may be at least ten percent higher than the validation accuracy, especially for later epochs."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#model-2-using-data-augmentation-flip-and-rotation",
    "href": "posts/bruin/HW5_resubmit/index.html#model-2-using-data-augmentation-flip-and-rotation",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "This next model will involve data augmentation layers. Data augmentation refers to the practice of including modified copies of the same image in the training set.\nFor example, flipped or rotated images are still the same image in the content they contain. Therefore, incorporating transformed versions of the image in the training process help the model learn these invariant features of the input images.\nWe begin by creating a keras.layers.RandomFlip() and keras.layers.RandomRotation() layer.\n\nfor images, labels in train_ds.take(1):\n    # Assuming you want the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n\n# Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\n\n\nThen, we create a plot of the original image and two more copies of that image where RandomFlip() has been applied.\n\ndata_augmentation_flipped = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n\n\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\nfor i in range(2):\n    augmented_image = data_augmentation_flipped(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThen, we create a plot of the original image and two more copies of that image where RandomRotation() has been applied.\n\ndata_augmentation_rotated = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n])\n\n\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\nfor i in range(2):\n    augmented_image = data_augmentation_rotated(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel2.summary()\n\nModel: \"sequential_8\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_7 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_2 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_9 (Conv2D)                    │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_6 (MaxPooling2D)       │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_10 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_7 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_11 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_3 (Flatten)                  │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (Dense)                      │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTo train a model on a dataset:\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.4742 - loss: 52.9159 - val_accuracy: 0.5868 - val_loss: 0.6810\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 225ms/step - accuracy: 0.5281 - loss: 1.4451 - val_accuracy: 0.6234 - val_loss: 0.6575\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 224ms/step - accuracy: 0.5251 - loss: 1.4545 - val_accuracy: 0.6432 - val_loss: 0.6396\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 231ms/step - accuracy: 0.5415 - loss: 1.4153 - val_accuracy: 0.6582 - val_loss: 0.6400\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.5409 - loss: 1.4078 - val_accuracy: 0.6543 - val_loss: 0.6211\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.5449 - loss: 1.3784 - val_accuracy: 0.6793 - val_loss: 0.5936\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5538 - loss: 1.3775 - val_accuracy: 0.6849 - val_loss: 0.5985\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.5570 - loss: 1.3586 - val_accuracy: 0.6651 - val_loss: 0.6062\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5465 - loss: 1.3940 - val_accuracy: 0.6935 - val_loss: 0.5869\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 230ms/step - accuracy: 0.5583 - loss: 1.3649 - val_accuracy: 0.6776 - val_loss: 0.6089\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5554 - loss: 1.3624 - val_accuracy: 0.6999 - val_loss: 0.6217\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5609 - loss: 1.3854 - val_accuracy: 0.7077 - val_loss: 0.5896\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5592 - loss: 1.3439 - val_accuracy: 0.6952 - val_loss: 0.6142\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5713 - loss: 1.3408 - val_accuracy: 0.6887 - val_loss: 0.5935\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.5612 - loss: 1.3488 - val_accuracy: 0.6862 - val_loss: 0.5874\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5692 - loss: 1.3433 - val_accuracy: 0.7059 - val_loss: 0.5640\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5623 - loss: 1.3483 - val_accuracy: 0.7102 - val_loss: 0.5722\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.5697 - loss: 1.3539 - val_accuracy: 0.6797 - val_loss: 0.5914\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 230ms/step - accuracy: 0.5318 - loss: 1.5118 - val_accuracy: 0.6109 - val_loss: 0.7261\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.5268 - loss: 1.4176 - val_accuracy: 0.6303 - val_loss: 0.6708\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 58.7% and 71% during training. Evidently, this model is able to consistently achieve at least 60% validation accuracy. Scores of 70% and above were also achieved.\nCompared to the validation accuracy of model1, model2 performed roughly the same as the lowest and highest accuracies were about 59-60% and 70-71%, respectively.\nIn model2, overfitting does not occur as the training accuracies are consistently lower than the validation accuracies."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#model-3-using-data-preprocessing",
    "href": "posts/bruin/HW5_resubmit/index.html#model-3-using-data-preprocessing",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "This next model will involve a preprocessing layer. This makes simple transformations to the input data.\nFor example, it may be the case that the original input image has pixels with RGB values between 0 and 255, but some models train faster with RGB values normalized between 0 and 1 (grayscale), or possibly between -1 and 1. Scaling the weights still result in mathematically identical situations. However, if the scaling process was handled prior to the training process, more of the training energy can be spent on the actual signal in the data rather than on the adjusting of the weights to the data scale.\nFor example, flipped or rotated images are still the same image in the content they contain. Therefore, incorporating transformed versions of the image in the training process help the model learn these invariant features of the input images.\n\n# all of the following found in the instructions\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = models.Sequential([\n    preprocessor,\n    # layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    # layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.RandomFlip(),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2), # number of classes\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel3.summary()\n\nModel: \"sequential_11\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_15 (Functional)           │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_10 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_5 (RandomRotation)   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_18 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_12 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_19 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_13 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_20 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_6 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (Dropout)                  │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (Dense)                     │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nTo train a model on a dataset:\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 223ms/step - accuracy: 0.7916 - loss: 0.4508 - val_accuracy: 0.7752 - val_loss: 0.4804\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7988 - loss: 0.4413 - val_accuracy: 0.7876 - val_loss: 0.4608\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 219ms/step - accuracy: 0.7954 - loss: 0.4402 - val_accuracy: 0.7915 - val_loss: 0.4551\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7963 - loss: 0.4391 - val_accuracy: 0.7932 - val_loss: 0.4609\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 234ms/step - accuracy: 0.7995 - loss: 0.4307 - val_accuracy: 0.7863 - val_loss: 0.4774\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.8069 - loss: 0.4216 - val_accuracy: 0.7756 - val_loss: 0.4769\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8025 - loss: 0.4239 - val_accuracy: 0.8061 - val_loss: 0.4425\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8117 - loss: 0.4163 - val_accuracy: 0.8027 - val_loss: 0.4322\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8095 - loss: 0.4162 - val_accuracy: 0.8048 - val_loss: 0.4488\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.8090 - loss: 0.4102 - val_accuracy: 0.8117 - val_loss: 0.4229\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8155 - loss: 0.4056 - val_accuracy: 0.8095 - val_loss: 0.4274\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.8182 - loss: 0.4026 - val_accuracy: 0.8087 - val_loss: 0.4480\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.8155 - loss: 0.3973 - val_accuracy: 0.8104 - val_loss: 0.4418\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 234ms/step - accuracy: 0.8141 - loss: 0.3956 - val_accuracy: 0.8044 - val_loss: 0.4529\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 238ms/step - accuracy: 0.8310 - loss: 0.3849 - val_accuracy: 0.8087 - val_loss: 0.4594\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8221 - loss: 0.3941 - val_accuracy: 0.8052 - val_loss: 0.4294\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 231ms/step - accuracy: 0.8214 - loss: 0.3831 - val_accuracy: 0.8005 - val_loss: 0.4657\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.8235 - loss: 0.3840 - val_accuracy: 0.8078 - val_loss: 0.4360\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.8274 - loss: 0.3761 - val_accuracy: 0.8229 - val_loss: 0.4012\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.8344 - loss: 0.3734 - val_accuracy: 0.8113 - val_loss: 0.4393\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 77.5% and 82.3% during training. Evidently, this model is able to consistently achieve at least 80% validation accuracy.\nCompared to the validation accuracy of model1, model3 performed at least 18% better, with the lowest accuracies performing roughly 19% better and the highest accuracies performing roughly 21% better.\nIn model3, overfitting does not occur as the training accuracies are overall lower than the validation accuracies."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#model-4-using-transfer-learning",
    "href": "posts/bruin/HW5_resubmit/index.html#model-4-using-transfer-learning",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "For this last model, we will be using a pre-existing model for the task of distinguishing between cats and dogs. Unlike the previous models, this one will not be trained from scratch. Instead, we will incorporate a pre-existing “base model” into the full model, which is ultimately what will be trained.\nThe benefit of using a pre-existing model is that someone might already have trained a model that performs a similar task, and this model could have already learned relevant patterns. This should lead us to seeing increased validation accuracy.\nThe following downloads MobileNetV3Large and configures it aas a layer to be included in this model.\n\n# all of the following provided in instructions\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v3.py:512: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n\nmodel4 will make use of MobileNetV3Large through incorporating base_model and IMG_SHAPE.\n\nmodel4 = models.Sequential([\n    # following two are the data augmentation layers from Part 3\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),  # Horizontal flip augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    base_model_layer, # constructed above\n    layers.GlobalMaxPool2D(), # additional layer in between Layer 3 and 4\n    layers.Dense(2) # Layer 4: Dense(2) layer to perform the classification\n])\n\nTrain the model for 20 epochs and plot the history of the accuracy on both the training and validation sets.\n\nmodel4.compile(optimizer = 'adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n              metrics = ['accuracy'])\n\n\nmodel4.summary()\n\nModel: \"sequential_12\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_11 (RandomFlip)          │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_6 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_18 (Functional)           │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_14 (Dense)                     │ (None, 2)                   │           1,922 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nClearly, there is a lot of complexity hidden in the base_model_layer. There are 1,922 parameters that are trainable out of almost three million total parameters, which is a significantly smaller ratio of trainable parameters compared to the other models.\nTo train a model on a dataset:\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 120s 661ms/step - accuracy: 0.7845 - loss: 1.8376 - val_accuracy: 0.9583 - val_loss: 0.3296\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 412ms/step - accuracy: 0.9025 - loss: 0.6569 - val_accuracy: 0.9570 - val_loss: 0.2781\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 419ms/step - accuracy: 0.9078 - loss: 0.5825 - val_accuracy: 0.9665 - val_loss: 0.2192\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 634ms/step - accuracy: 0.9132 - loss: 0.4687 - val_accuracy: 0.9660 - val_loss: 0.1860\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 632ms/step - accuracy: 0.9208 - loss: 0.3738 - val_accuracy: 0.9471 - val_loss: 0.2629\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 629ms/step - accuracy: 0.9141 - loss: 0.4168 - val_accuracy: 0.9574 - val_loss: 0.1658\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 633ms/step - accuracy: 0.9240 - loss: 0.3267 - val_accuracy: 0.9592 - val_loss: 0.1789\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9204 - loss: 0.3412 - val_accuracy: 0.9355 - val_loss: 0.3163\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9210 - loss: 0.3376 - val_accuracy: 0.9536 - val_loss: 0.2004\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 638ms/step - accuracy: 0.9243 - loss: 0.2820 - val_accuracy: 0.9454 - val_loss: 0.2445\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 59s 407ms/step - accuracy: 0.9164 - loss: 0.3396 - val_accuracy: 0.9420 - val_loss: 0.2534\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 630ms/step - accuracy: 0.9270 - loss: 0.2851 - val_accuracy: 0.9549 - val_loss: 0.1886\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 408ms/step - accuracy: 0.9106 - loss: 0.3796 - val_accuracy: 0.9561 - val_loss: 0.1964\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 416ms/step - accuracy: 0.9262 - loss: 0.2683 - val_accuracy: 0.9613 - val_loss: 0.1323\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 421ms/step - accuracy: 0.9236 - loss: 0.2740 - val_accuracy: 0.9364 - val_loss: 0.2902\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 415ms/step - accuracy: 0.9214 - loss: 0.2795 - val_accuracy: 0.9544 - val_loss: 0.1839\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 416ms/step - accuracy: 0.9293 - loss: 0.2429 - val_accuracy: 0.9540 - val_loss: 0.1685\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 414ms/step - accuracy: 0.9177 - loss: 0.2666 - val_accuracy: 0.9445 - val_loss: 0.2345\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 413ms/step - accuracy: 0.9192 - loss: 0.3048 - val_accuracy: 0.9506 - val_loss: 0.2221\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 633ms/step - accuracy: 0.9274 - loss: 0.2557 - val_accuracy: 0.9321 - val_loss: 0.3122\n\n\nVisualize the training history:\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe accuracy of my model stabilized between roughly 93.2% and 96.7% during training. Evidently, this model is able to consistently achieve at least 93% validation accuracy.\nCompared to the validation accuracy of model1, model4 performed at least 35% better, with the lowest and highest accuracies performing roughly 35% better.\nIn model4, overfitting does not occur as the training accuracies are consistently lower than the validation accuracies."
  },
  {
    "objectID": "posts/bruin/HW5_resubmit/index.html#part-6-accuracy-score-on-test-data",
    "href": "posts/bruin/HW5_resubmit/index.html#part-6-accuracy-score-on-test-data",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "The most performant model is model4which consistently achieves at elast 93% validation accuracy!\nNow, we will evaluate the accuracy of this model on the unseen test_ds.\n\nmodel4.evaluate(test_ds)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 11s 288ms/step - accuracy: 0.9756 - loss: 0.0746\n\n\n[0.07890091091394424, 0.9733448028564453]\n\n\nBy evaluating model4 on the unseen test_ds, it can be seen that this model performs quite well at around 98% accuracy!"
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html",
    "href": "posts/bruin/HW4_copy/index.html",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\nimport time\n\n\nN = 101\nepsilon = 0.2\nnum_iters = 2700\n\n\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\nWe begin with matrix-vector multiplication to simulate the heat diffusion in the 2D space. To created the vector, the current solution\n\n\n\nScreenshot 2024-02-23 133351.png\n\n\nmust be flattened. This means that we view the summation as the element with index N x i + j in a vector of length N^2.\nThe matrix A has the size of N^2 * N^2 without all-zero rows or all-zero columns. This is defined in the function get_A(N), which takes in the value N as the argument and returns the corresponding matrix A.\n\n# from heat_equation import get_A\n# import inspect\n# print(inspect.getsource(get_A))\n\n\ndef get_A(N):\n    \"\"\"\n    Creates a 2D finite difference matrix with adjusted diagonal entries\n    Args:\n        N: Size of array\n    Returns:\n        N x N updating matrix\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n# from heat_equation import advance_time_matvecmul\n# import inspect\n# print(inspect.getsource(advance_time_matvecmul))\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nA timer is used to time how long the simulation takes to run. We use both get_A(N) and advance_time_matvecmul() with u being updated in each iteration. For each simulation, we will run it for 2700 iterations (defined earlier as num_iters). For every 300 iterations, a visualization is created to display the heat diffusion. Together, each subplot is appended to a 3x3 grid of 2D heatmaps.\n\nA = get_A(N)\nu = u0\n\ngetA_plots = []\n\n# start the timer right before the for loop\nstarttime = time.time()\n\n# i starts at 1 because we do not want 0 and ends at num_iters + 1 because\n# we want the 2700th iteration (the last value in the for loop range is\n# not accounted for)\nfor i in range(1, num_iters + 1):\n    u = advance_time_matvecmul(A, u, epsilon)\n    # every 300th iteration\n    if i % 300 == 0 :\n        # add the plots of each iteration to the empty array\n        getA_plots.append(u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nRun Time: 154.8682155609131 seconds\n\n\nEvidently, the run time being almost 214 seconds is quite long. There must be a faster way to to generate the matrix. Later on, we will see more optimized performance with other simulations.\nThis visualization is used in each of the following types of simulations.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(getA_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count + 1) * 300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis next visualization uses sparse matrix data structures from the JAX package to exploit the zeros in the matrix. We will begin by importing sparse and jit.\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nfrom jax import grad, jit\n\nModuleNotFoundError: No module named 'jax'\n\n\nWe will use the batched coordinate (BCOO) format, which is the main high-level sparse object currently available in JAX. This offers a compressed storage format compatible with JAX transformations, specifically JIT. Now, we will crreate a sparse array from a dense array. Since we already created get_A() to create the finite difference matrix, we now just need to call this function to create the array A and convert it to the a JAX numpy sparse array.\nget_sparse_A(N) returns A_sp_matrix which is the matrix A in sparse format given N.\n\n# from heat_equation import get_sparse_A\n# import inspect\n# print(inspect.getsource(get_sparse_A))\n\n\ndef get_sparse_A(N):\n    \"\"\"\n    Advances the simulation using sparse matrix data structures from JAX batched\n    coordinate (BCOO)\n    Args:\n        N: Size of array\n    Returns:\n        A_sp_matrix which is the matrix A in a sparse format\n    \"\"\"\n    # the original dense array\n    A = get_A(N)\n    jnp_A = jnp.array(A)\n    # converted into sparse array\n    A_sp_matrix = sparse.BCOO.fromdense(jnp_A)\n    return A_sp_matrix\n\nWe also use a jit-ed version of advance_time_matvecmul that takes in the sparse matrix.\n\nA_sparse = get_sparse_A(N)\nu = u0\n\nsparse_plots = []\n\n# jitted version of advance_time_matvecmul\njitted_sparse = jax.jit(advance_time_matvecmul)\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    u = jitted_sparse(A_sparse, u, epsilon)\n    if i % 300 == 0 :\n        sparse_plots.append(u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nThis simulation was much faster at roughly just under 2 seconds. This is much faster than that of Part 1 where it was a at least 10x faster. We have found a simulation with more optimized performance!\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(sparse_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFor this method, we will manually update u by using vectorized array operations. Therefore, instead of having our advanced_time function having three parameters, it will only require the two following: u and epsilon.\n\n# from heat_equation import advance_time_numpy\n# import inspect\n# print(inspect.getsource(advance_time_numpy))\n\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep using np.roll()\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # padded version of u with zeroes along each axis\n    padded_u = np.pad(u, 1, mode = 'constant')\n\n    # rolled version of the padded 0 (meaning the main content gets shifted\n    # along each axis)\n    right_roll = np.roll(padded_u, 1, axis = 1)\n    left_roll = np.roll(padded_u, -1, axis = 1)\n    up_roll = np.roll(padded_u, -1, axis = 0)\n    down_roll = np.roll(padded_u, 1, axis = 0)\n\n    # updated u based on the summation equation from above\n    numpy_u = padded_u + epsilon * (right_roll + left_roll + up_roll + down_roll - (4 * padded_u))\n\n    # the returned solution still NxN (meaning padded zeroes are removed)\n    numpy_u_sliced = numpy_u[1:-1, 1:-1]\n\n    return numpy_u_sliced\n\n\nnumpy_u = u0\nnumpy_plots = []\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    numpy_u = advance_time_numpy(numpy_u, epsilon)\n    if i % 300 == 0:\n        numpy_plots.append(numpy_u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nWow! The vectorized array opertion using np.roll() is even faster than the two previous at roughly just under 1 second! Part 3 is at least 100x faster than Part 1.\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(numpy_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFor this last method, we will once again use the JAX and jitted version of a previous function similar to the sparse matrix method. Here, we will not use (sparse) matrix multiplication, which will optimize performance.\nadvance_time_jax() is the same as advance_time_numpy() except we replace all numpy with JAX numpy, or jnp.\n\n# from heat_equation import advance_time_jax\n# import inspect\n# print(inspect.getsource(advance_time_jax))\n\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep using jax numpy.roll()\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # padded version of u with zeroes along each axis\n    padded_u = jnp.pad(u, 1, mode = 'constant')\n\n    # rolled version of the padded 0 (meaning the main content gets\n    # shifted along each axis)\n    right_roll = jnp.roll(padded_u, 1, axis = 1)\n    left_roll = jnp.roll(padded_u, -1, axis = 1)\n    up_roll = jnp.roll(padded_u, -1, axis = 0)\n    down_roll = jnp.roll(padded_u, 1, axis = 0)\n\n    # updated u based on the summation equation from above\n    jnp_u = padded_u + epsilon *(right_roll + left_roll + up_roll + down_roll - (4 * padded_u))\n\n    # the returned solution still NxN (meaning padded zeroes are removed)\n    jnp_u_sliced = jnp_u[1:-1, 1:-1]\n\n    return jnp_u_sliced\n\n\njnp_u = u0\njnp_plots = []\n\njitted_jax = jax.jit(advance_time_jax)\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    jnp_u = jitted_jax(jnp_u, epsilon)\n    if i % 300 == 0:\n        jnp_plots.append(jnp_u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nWOW! This simulation uses the same structure as advance_time_numpy() to do jit compilation. The resulting run time is aroud 0.18 seconds. Part 4 is at least two times faster than Part 3.\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\ncount = 0\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(jnp_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nFrom fastest to slowest, the simulations ranked are:\n\nVectorized array with JAX just-in-time (Part 4)\nVectorized array with numpy roll (Part 3)\nSparse matrix using JAX (Part 2)\nMatrix-vector mulitplication (Part 1)\n\nTherefore, vectorized array with JAX just-in-time (Part 4) had the fastest performance of the four methods. However, the easiest for me to write was the sparse matrix method (from Part 2) as it involved two critical lines including BCOO from the numpy documentation provided in lecture."
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html#part-1-matrix-multiplication",
    "href": "posts/bruin/HW4_copy/index.html#part-1-matrix-multiplication",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "We begin with matrix-vector multiplication to simulate the heat diffusion in the 2D space. To created the vector, the current solution\n\n\n\nScreenshot 2024-02-23 133351.png\n\n\nmust be flattened. This means that we view the summation as the element with index N x i + j in a vector of length N^2.\nThe matrix A has the size of N^2 * N^2 without all-zero rows or all-zero columns. This is defined in the function get_A(N), which takes in the value N as the argument and returns the corresponding matrix A.\n\n# from heat_equation import get_A\n# import inspect\n# print(inspect.getsource(get_A))\n\n\ndef get_A(N):\n    \"\"\"\n    Creates a 2D finite difference matrix with adjusted diagonal entries\n    Args:\n        N: Size of array\n    Returns:\n        N x N updating matrix\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n# from heat_equation import advance_time_matvecmul\n# import inspect\n# print(inspect.getsource(advance_time_matvecmul))\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nA timer is used to time how long the simulation takes to run. We use both get_A(N) and advance_time_matvecmul() with u being updated in each iteration. For each simulation, we will run it for 2700 iterations (defined earlier as num_iters). For every 300 iterations, a visualization is created to display the heat diffusion. Together, each subplot is appended to a 3x3 grid of 2D heatmaps.\n\nA = get_A(N)\nu = u0\n\ngetA_plots = []\n\n# start the timer right before the for loop\nstarttime = time.time()\n\n# i starts at 1 because we do not want 0 and ends at num_iters + 1 because\n# we want the 2700th iteration (the last value in the for loop range is\n# not accounted for)\nfor i in range(1, num_iters + 1):\n    u = advance_time_matvecmul(A, u, epsilon)\n    # every 300th iteration\n    if i % 300 == 0 :\n        # add the plots of each iteration to the empty array\n        getA_plots.append(u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nRun Time: 154.8682155609131 seconds\n\n\nEvidently, the run time being almost 214 seconds is quite long. There must be a faster way to to generate the matrix. Later on, we will see more optimized performance with other simulations.\nThis visualization is used in each of the following types of simulations.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(getA_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count + 1) * 300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html#part-2-sparse-matrix-using-jax",
    "href": "posts/bruin/HW4_copy/index.html#part-2-sparse-matrix-using-jax",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "This next visualization uses sparse matrix data structures from the JAX package to exploit the zeros in the matrix. We will begin by importing sparse and jit.\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nfrom jax import grad, jit\n\nModuleNotFoundError: No module named 'jax'\n\n\nWe will use the batched coordinate (BCOO) format, which is the main high-level sparse object currently available in JAX. This offers a compressed storage format compatible with JAX transformations, specifically JIT. Now, we will crreate a sparse array from a dense array. Since we already created get_A() to create the finite difference matrix, we now just need to call this function to create the array A and convert it to the a JAX numpy sparse array.\nget_sparse_A(N) returns A_sp_matrix which is the matrix A in sparse format given N.\n\n# from heat_equation import get_sparse_A\n# import inspect\n# print(inspect.getsource(get_sparse_A))\n\n\ndef get_sparse_A(N):\n    \"\"\"\n    Advances the simulation using sparse matrix data structures from JAX batched\n    coordinate (BCOO)\n    Args:\n        N: Size of array\n    Returns:\n        A_sp_matrix which is the matrix A in a sparse format\n    \"\"\"\n    # the original dense array\n    A = get_A(N)\n    jnp_A = jnp.array(A)\n    # converted into sparse array\n    A_sp_matrix = sparse.BCOO.fromdense(jnp_A)\n    return A_sp_matrix\n\nWe also use a jit-ed version of advance_time_matvecmul that takes in the sparse matrix.\n\nA_sparse = get_sparse_A(N)\nu = u0\n\nsparse_plots = []\n\n# jitted version of advance_time_matvecmul\njitted_sparse = jax.jit(advance_time_matvecmul)\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    u = jitted_sparse(A_sparse, u, epsilon)\n    if i % 300 == 0 :\n        sparse_plots.append(u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nThis simulation was much faster at roughly just under 2 seconds. This is much faster than that of Part 1 where it was a at least 10x faster. We have found a simulation with more optimized performance!\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(sparse_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html#part-3-direct-operation-with-numpy",
    "href": "posts/bruin/HW4_copy/index.html#part-3-direct-operation-with-numpy",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "For this method, we will manually update u by using vectorized array operations. Therefore, instead of having our advanced_time function having three parameters, it will only require the two following: u and epsilon.\n\n# from heat_equation import advance_time_numpy\n# import inspect\n# print(inspect.getsource(advance_time_numpy))\n\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep using np.roll()\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # padded version of u with zeroes along each axis\n    padded_u = np.pad(u, 1, mode = 'constant')\n\n    # rolled version of the padded 0 (meaning the main content gets shifted\n    # along each axis)\n    right_roll = np.roll(padded_u, 1, axis = 1)\n    left_roll = np.roll(padded_u, -1, axis = 1)\n    up_roll = np.roll(padded_u, -1, axis = 0)\n    down_roll = np.roll(padded_u, 1, axis = 0)\n\n    # updated u based on the summation equation from above\n    numpy_u = padded_u + epsilon * (right_roll + left_roll + up_roll + down_roll - (4 * padded_u))\n\n    # the returned solution still NxN (meaning padded zeroes are removed)\n    numpy_u_sliced = numpy_u[1:-1, 1:-1]\n\n    return numpy_u_sliced\n\n\nnumpy_u = u0\nnumpy_plots = []\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    numpy_u = advance_time_numpy(numpy_u, epsilon)\n    if i % 300 == 0:\n        numpy_plots.append(numpy_u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nWow! The vectorized array opertion using np.roll() is even faster than the two previous at roughly just under 1 second! Part 3 is at least 100x faster than Part 1.\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\n# 3x3 grid of 2D heatmaps\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\n# to keep track of which element in the grid (0-8)\ncount = 0\n# accounting for both axes\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(numpy_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html#part-4-jax-and-just-in-time-compilation",
    "href": "posts/bruin/HW4_copy/index.html#part-4-jax-and-just-in-time-compilation",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "For this last method, we will once again use the JAX and jitted version of a previous function similar to the sparse matrix method. Here, we will not use (sparse) matrix multiplication, which will optimize performance.\nadvance_time_jax() is the same as advance_time_numpy() except we replace all numpy with JAX numpy, or jnp.\n\n# from heat_equation import advance_time_jax\n# import inspect\n# print(inspect.getsource(advance_time_jax))\n\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the simulation by one timestep using jax numpy.roll()\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    # padded version of u with zeroes along each axis\n    padded_u = jnp.pad(u, 1, mode = 'constant')\n\n    # rolled version of the padded 0 (meaning the main content gets\n    # shifted along each axis)\n    right_roll = jnp.roll(padded_u, 1, axis = 1)\n    left_roll = jnp.roll(padded_u, -1, axis = 1)\n    up_roll = jnp.roll(padded_u, -1, axis = 0)\n    down_roll = jnp.roll(padded_u, 1, axis = 0)\n\n    # updated u based on the summation equation from above\n    jnp_u = padded_u + epsilon *(right_roll + left_roll + up_roll + down_roll - (4 * padded_u))\n\n    # the returned solution still NxN (meaning padded zeroes are removed)\n    jnp_u_sliced = jnp_u[1:-1, 1:-1]\n\n    return jnp_u_sliced\n\n\njnp_u = u0\njnp_plots = []\n\njitted_jax = jax.jit(advance_time_jax)\n\n# start the timer right before the for loop\nstarttime = time.time()\n\nfor i in range(1, num_iters + 1):\n    jnp_u = jitted_jax(jnp_u, epsilon)\n    if i % 300 == 0:\n        jnp_plots.append(jnp_u.copy())\n\n# run_time is current time - starttime\nrun_time = time.time() - starttime\n\n# print total time\nprint(\"Run Time: \" + str(run_time) + \" seconds\")\n\nWOW! This simulation uses the same structure as advance_time_numpy() to do jit compilation. The resulting run time is aroud 0.18 seconds. Part 4 is at least two times faster than Part 3.\nOnce again, we will display the 3x3 grid of heatmaps correponding to this simulation.\n\nfig, ax = plt.subplots(3, 3, figsize = (12, 12))\ncount = 0\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(jnp_plots[count])\n        ax[i][j].set_title(f\"Iteration {(count+1)*300}\")\n        count += 1\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/HW4_copy/index.html#part-5-comparison-of-methods",
    "href": "posts/bruin/HW4_copy/index.html#part-5-comparison-of-methods",
    "title": "Homework 4: Matrix Multiplication and JAX/Numpy",
    "section": "",
    "text": "From fastest to slowest, the simulations ranked are:\n\nVectorized array with JAX just-in-time (Part 4)\nVectorized array with numpy roll (Part 3)\nSparse matrix using JAX (Part 2)\nMatrix-vector mulitplication (Part 1)\n\nTherefore, vectorized array with JAX just-in-time (Part 4) had the fastest performance of the four methods. However, the easiest for me to write was the sparse matrix method (from Part 2) as it involved two critical lines including BCOO from the numpy documentation provided in lecture."
  },
  {
    "objectID": "posts/bruin/HW4/index_Thomas.html",
    "href": "posts/bruin/HW4/index_Thomas.html",
    "title": "Homework 4: Matrix Multiplication and Jax Numpy (Heat Diffusion)",
    "section": "",
    "text": "In this blog post, we will be going over how to use numpy arrays as well as comparing special tools to work with linear algebra. For our example, we will be conducting a simulation of two-dimensional heat diffusion maps.\nBecause we will be working with numpy as well as data visualization, we will need to import both numpy as well as pyplot.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFor our 2D-heat diffusion maps, we will be using the following parameters:\n\nN = 101\nepsilon = 0.2\niters = 2700\n\nHere, our N represents the size of our array (meaning there will be N^2 values in our array) epsilon represents the “heaviness / stability” of each of our update, and iters represents the amount of iterations our simulation will run.\nFirst, we will need to create an an initial heatmap to iterate and update upon. We can create an initial N x N empty grid with a singular unit of heat at the center with the following line of code:\n\nu0 = np.zeros((N, N))              # Creating NxN numpy array with entries of 0\nu0[int(N/2), int(N/2)] = 1.0       # Assigning the midpoint value of initial array to 1\nplt.imshow(u0)                     # Plotting numpy array with the 1 unit of heat at midpoint\n\n\n\n\n\n\n\n\nGreat! Now that we have our initical condition heatmap, we can using different methods to update it.\n\nMatrix Multiplication\nThe first method we will use is matrix multiplication. Essentially, we want to construct an updating matrix A that will act upon the flattened version of u which will be iterated and updated each time.\nThus, the first function we want to create is for constructing our update matrix A (this matrix is called a finite difference matrix). We want our update in discrete time to be represented by the following function:\n\n\n\nScreenshot 2024-02-23 1.31.17 PM.png\n\n\nThus, we want our updating matrix A to represent the paranthesis part that is multiplied to epsilon.\n\n\n\nScreenshot 2024-02-23 1.30.29 PM.png\n\n\nHere, each u will represent a diagonal in our matrix: * u[i+1][j]: represents nth upper diagonal * u[i-1][j]: represents nth lower diagonal * u[i][j+1]: represents 1st upper diagonal * u[i][j-1]: represents 1st lower diagonal * u[i][j]: main diagonal\nWith this information, we can now create our function to account for the desired grid size N, which should look something like this:\n\n# import inspect\n# from heat_equation import get_A\n# print(inspect.getsource(get_A))\n\n\ndef get_A(N):\n  \"\"\"Creates a 2D finite difference matrix by adjusting the diagonal entries\n  Args:\n       N: Integer representing size of desired matrix\n\n  Returns:\n       N x N adjusting matrix.\n  \"\"\"\n  # Getting total number of grid points\n  n = N * N\n  # Creating list with representative diagonals\n  diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n\n  # Adjusting diagonals to account periodic 0's\n  diagonals[1][(N-1)::N] = 0\n  diagonals[2][(N-1)::N] = 0\n\n  # Constructing finite difference matrix\n  A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n  return A\n\nNow that we have our function, let’s see what our function looks like with a test value parameter. For simplicity, we will use 3.\n\nget_A(3)\n\narray([[-4.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1., -4.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  1., -4.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0., -4.,  1.,  0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  1., -4.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  1., -4.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.,  0.,  0., -4.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  1., -4.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1., -4.]])\n\n\nOur code outputted the matrix that we were looking for. Now, we can start on our next step: updating u.\nTo match the discrete time update equation, we want to take into account 3 parameters: our update matrix A, the iterated matrix u, and our “scaling/stability” value epsilon. Because we already have all the components of the equation. We can just type out the equation in our function. So our function should look something like this:\n\n# from heat_equation import advance_time_matvecmul\n# import inspect\n# print(inspect.getsource(advance_time_matvecmul))\n\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]                                        # Calculating size of grid\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))   # Flattening 2D grid 'u', Matrix mult. with\n                                                          # A, Reshaping back to original grade shape,\n                                                          # Updating 'u' with matrix multiplication + epsilon\n    return u\n\nWith both of these equations, we can now run our iterations.\nFirst, because we are comparing different methods for creating identical heatmaps, we want to import time in order to see execution time.\n\nimport time\n\nNext, we want to create each of our variable in order to be able to use the advance_time_matvecmul function. We already have epsilon established earlier, so we just need A and u. Because we want to see the development of the grid with each update, we want to create a list to append the u at different iterations. Thus, we will use the following block of code:\n\nA = get_A(N)\nu = u0\nu_array = []\n\n# Starting execution time for iterations\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  u = advance_time_matvecmul(A, u, epsilon)             # Updating our heatmap grid\n  if i % 300 == 0:\n    u_array.append(u.copy())                            # Appending every 300-th heatmap grid\n\n# Calculating iteration time\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\nExecution Time is  233.45097470283508  seconds\n\n\nThis approach took quite a long time, taking almost 4 minutes to go through 2700 iterations.\nAlso note that through the conditional statement, we only appended 9 versions of u. With this appended list, we can now visualize the diffusion heatmaps. Because we have 9 grids, we can do 3x3 subplots by using pyplot through the following block of code:\n\ndef diffusion_visualization(list):\n  \"\"\"Generating a visualization of the 9 developed heatmap grids\n  Args:\n      list: list of appended heatmap grids\n\n  Returns:\n      3 rows, 3 columns of subplots of the heatmap grids\n  \"\"\"\n  # Generating base subplots\n  fig, ax = plt.subplots(3, 3, figsize = (12, 12))\n\n  # Creating indexing counter for list\n  count = 0\n\n  # Looping through row\n  for i in range(3):\n    # Looping through column\n    for j in range(3):\n      # Plotting/Showing the nth item in the list\n      ax[i][j].imshow(list[count])\n      # Setting title of the nth item in the list\n      ax[i][j].set_title(f\"{(count+1)*300}th iteration\")\n      # Iterator\n      count += 1\n\n  plt.tight_layout()\n  plt.show()\n\nGreat! Now that we have our diffusion heat visualization function, let’s run it using our list of wanted iterations.\n\ndiffusion_visualization(u_array)\n\n\n\n\n\n\n\n\nNotice that as more iterations occur, the heatmap develops, becoming more vibrant with the circles getting wider (exactly what we want).\n\n\nSparse Matrix in JAX\nNow that we have our initial method down, let’s use another method to increase computational time. For this one, we will use sparse matrices from JAX, a Python library designed for high-performance numerical computing.\nSo first, we need to import the following:\n\nimport jax\nfrom jax.experimental import sparse           # Allowing us to create sparse matrices\nimport jax.numpy as jnp                       # Allowing us to utilize jax numpy\nfrom jax import jit                           # Allowing us to use Just-In-Time compilations\n                                              # to execute JAX function effectively\n\n\nThe next step we want to do is create a function to output us a sparse matrix. Because we already have the function get_A to help us output a finite difference matrix, we can use call get_A in this function to create the array A, which we will then turn into a JAX numpy sparse array. We can do that with the following block of code:\n\n# from heat_equation import get_sparse_A\n# import inspect\n# print(inspect.getsource(get_sparse_A))\n\n\ndef get_sparse_A(N):\n  \"\"\" Advancing simulation through BCOO (JAX batched coordimate)\n  to create sparse updating matrix\n\n  Args:\n    N: size of array\n  Returns:\n    an N x N sparse updating matrix\n  \"\"\"\n\n  # Creating updating matrix\n  A = get_A(N)\n  # Transforming A into JAX numpy array\n  jnp_A = jnp.array(A)\n  # Converting into sparse matrix\n  A_sp_matrix = sparse.BCOO.fromdense(jnp_A)\n  return A_sp_matrix\n\nNow that we have our new sparse matrix, we can repeat the process of iteration and visualization. However, we also want to use the jit-ed version of advance_time_matvecmul to execute the JAX numpys for effectively. Thus our code should look something like this:\n\n# Getting elements for advance_time function and iteration for loop\nA_sp_matrix = get_sparse_A(N)\nu = u0\nu_array_sparse = []\n\n# Using JIT compilation for advance_time function\njitted_advance_time_matvecmul = jax.jit(advance_time_matvecmul)\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  u = jitted_advance_time_matvecmul(A_sp_matrix, u, epsilon)\n  if i % 300 ==0:\n    u_array_sparse.append(u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(u_array_sparse)\n\nExecution Time is  1.3260242938995361  seconds\n\n\n\n\n\n\n\n\n\nNotice that this function is MUCH faster than the first process! It took only 1.33 seconds.\n\n\nDirect Operation with Numpy\nFor this method, rather than relying on an updating matrix, we can update matrix u itself by using numpy’s vectorized array operations.\nTherefore, we will create an updated version of advance_time_vatmecmul that only needs two parameters: u and epsilon.\nAgain, we want our function to execute the updating equation above. Except this time. Rather than writing a matrix to represent the paranthetical part. we can use numpy to represent each u[i+1][j], u[i-1][j], u[i][j+1], u[i][j-1].\nWe can do this with using np.roll(), where we shift each of the points either left/right or up/down, depending on which we need. However, an important factor of np.roll() that we need to know is that it also takes the last point in the array and makes it the first and vice versa, which we don’t want. Thus we need to pad our array with 0 along the edge to prevent this. Thus our code should look something like this:\n\n# from heat_equation import advance_time_numpy\n# import inspect\n# print(inspect.getsource(advance_time_numpy))\n\n\ndef advance_time_numpy(u, epsilon):\n  \"\"\"Advances the simulation by one timestep, via numpy rolling\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n  \"\"\"\n\n  # Creating padded version of u with 0's along edge\n  padded_u = np.pad(u, 1, mode = 'constant')\n\n  # Creating rolled version of padded u\n  right_roll = np.roll(padded_u, 1, axis = 1)\n  left_roll = np.roll(padded_u, -1, axis = 1)\n  up_roll = np.roll(padded_u, -1, axis = 0)\n  down_roll = np.roll(padded_u, 1, axis = 0)\n\n  # Updating u\n  numpy_u = padded_u + epsilon*(right_roll + left_roll + up_roll + down_roll - (4*padded_u))\n  # Removing the 0's on the edge\n  numpy_u_sliced = numpy_u[1:-1, 1:-1]\n  return numpy_u_sliced\n\nNotice that before returning the updated u, we used sliced indexing because the padded u is a (N+2) x (N+2) array, but we only want a N x N array.\nWith this function, we can again run our iterations and visualization and see how long this takes.\n\nnumpy_u = u0\nresults_numpy = []\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  numpy_u = advance_time_numpy(numpy_u, epsilon)\n  if i % 300 ==0:\n    results_numpy.append(numpy_u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(results_numpy)\n\nExecution Time is  0.522552490234375  seconds\n\n\n\n\n\n\n\n\n\nThis time, it only takes 0.52 seconds, 2.3 times as fast!\n\n\nJAX\nThe last method we will use is jax, similar to when we did sparse matrix, we will do an jax version of the previous function, which will allow us to use just-in-time compilation. Benefit of this method as well is that we will not need to rely on (sparse) matrix multiplication.\nEverything will be the same as advance_time_numpy, but we just replace all the numpy with jax.numpy, or jnp. Thus we should get the following code:\n\n# from heat_equation import advance_time_jax\n# import inspect\n# print(inspect.getsource(advance_time_jax))\n\n\ndef advance_time_jax(u, epsilon):\n  \"\"\"Advances the simulation by one timestep, via jax numpy rolling\n  Args:\n      u: N x N grid state at timestep k.\n      epsilon: stability constant.\n\n  Returns:\n      N x N Grid state at timestep k+1.\n  \"\"\"\n\n  # Creating padded version of u with 0's along edge\n  padded_u = jnp.pad(u, 1, mode = 'constant')\n\n  # Creating rolled version of padded u\n  right_roll = jnp.roll(padded_u, 1, axis = 1)\n  left_roll = jnp.roll(padded_u, -1, axis = 1)\n  up_roll = jnp.roll(padded_u, -1, axis = 0)\n  down_roll = jnp.roll(padded_u, 1, axis = 0)\n\n  # Updating u\n  jnp_u = padded_u + epsilon*(right_roll + left_roll + up_roll + down_roll - (4*padded_u))\n  # Removing the 0's on the edge\n  jnp_u_sliced = jnp_u[1:-1, 1:-1]\n  return jnp_u_sliced\n\nFor one last time, we will run our iteration and visualization. Because we are dealing with jax numpy again, we will use the jit-ed version of advance_time_jax.\n\njitted_jax = jax.jit(advance_time_jax)\njnp_u = u0\nresults_jnp = []\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  jnp_u = jitted_jax(jnp_u, epsilon)\n  if i % 300 ==0:\n    results_jnp.append(jnp_u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(results_jnp)\n\nExecution Time is  0.08782696723937988  seconds\n\n\n\n\n\n\n\n\n\nThis one is the fastest of them all, taking 0.09 seconds, more than 5 times as fast as the previous one!\n\n\nComparison\nOut of all of these, the fastest is of course with jax as it took only 0.09 seconds, followed by direct operation through numpy, then sparse matrix, and lastly direct matrix multiplication.\nOut of all of these, I think that direct operation through nunmpy was the easiest to write since it did not require me to write two functions, and I did not have to use an additional updating matrix, which got really complicated."
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html",
    "href": "posts/bruin/HW2_copy/Untitled.html",
    "title": "HW2",
    "section": "",
    "text": "This activity aims to find the works of actors in a certain movie only under a specific category: “Acting.” Some actors may also hold other positions on other teams, such as a stunt double on “Crew” or an assistant director on “Production,” but this activity is only interested in the “Acting” positions of the “actors” within a specific movie.\nIn the following tutorial, I will be demonstrating the webscraping process using the TMDB sites of two movies: first from the example movie, Harry Potter and the Philosopher’s Stone, and later from my favorite movie, Kill Bill: Vol. 1.\n\n\n\n\nMy scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper.\n\n\n\n\n\nA file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\n\n\n\n\n\n\nThis is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns\n\n\n\n\n\n\n\n\n\nThis bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#part-1-initial-steps",
    "href": "posts/bruin/HW2_copy/Untitled.html#part-1-initial-steps",
    "title": "HW2",
    "section": "",
    "text": "My scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper."
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "href": "posts/bruin/HW2_copy/Untitled.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "title": "HW2",
    "section": "",
    "text": "A file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "href": "posts/bruin/HW2_copy/Untitled.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "title": "HW2",
    "section": "",
    "text": "This is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns"
  },
  {
    "objectID": "posts/bruin/HW2_copy/Untitled.html#data-visualization",
    "href": "posts/bruin/HW2_copy/Untitled.html#data-visualization",
    "title": "HW2",
    "section": "",
    "text": "This bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/hw2_resubmit.html",
    "href": "posts/bruin/HW2_copy/hw2_resubmit.html",
    "title": "HW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping",
    "section": "",
    "text": "This activity aims to find the works of actors in a certain movie only under a specific category: “Acting.” Some actors may also hold other positions on other teams, such as a stunt double on “Crew” or an assistant director on “Production,” but this activity is only interested in the “Acting” positions of the “actors” within a specific movie.\nIn the following tutorial, I will be demonstrating the webscraping process using the TMDB sites of two movies: first from the example movie, Harry Potter and the Philosopher’s Stone, and later from my favorite movie, Kill Bill: Vol. 1.\n\n\n\n\nMy scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browser that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper.\n\n\n\n\n\nA file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThe first method begins on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\nWhat is a Scrapy request?\nScrapy uses Requst objects, as well as Response, for crawling websites. Request objects are generated in the spiders and executed by the Downloader to create a Response.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis second method starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\nA for loop is used to go through each actor within the “Cast” section and arrives at each actor’s url to their personal page.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThe third method starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\n\n\n\nThe following dataframes containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Vol. 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill Vol. 1 actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# sort by movies with the most to least number of shared actors\nshared_actors = shared_actors.sort_values(by = \"Number of Shared Actors\",\nascending = False)\n\nshared_actors.head(50)\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n60\nKill Bill: Vol. 1\n38\n\n\n36\nKill Bill: The Whole Bloody Affair\n38\n\n\n57\nKill Bill: Vol. 2\n32\n\n\n14\nQT8: The First Eight\n7\n\n\n160\nIchi the Killer\n5\n\n\n496\nYonimo Kimyou na Monogatari Tokubetsuhen\n5\n\n\n2580\nHero\n4\n\n\n59\nThe Making of 'Kill Bill Vol. 1'\n4\n\n\n2208\nThe Supporting Actors in Byplaywood\n4\n\n\n56\nThe Making of 'Kill Bill Vol. 2'\n4\n\n\n76\nThe View\n4\n\n\n2511\nSignal\n3\n\n\n144\nSolitary Gourmet\n3\n\n\n94\nLIVE with Kelly and Mark\n3\n\n\n1890\nSwing Girls\n3\n\n\n1891\nThe Taste of Tea\n3\n\n\n1150\nCSI: Crime Scene Talks\n3\n\n\n1122\nBlade of the Immortal\n3\n\n\n1368\nBeing Michael Madsen\n3\n\n\n454\nAIBOU: Tokyo Detective Duo\n3\n\n\n165\nDjango Unchained\n3\n\n\n448\nAlive\n3\n\n\n447\n9 Souls\n3\n\n\n438\nGodzilla: Final Wars\n3\n\n\n2002\nCackling Kitarou\n3\n\n\n198\nThe X-Files\n3\n\n\n211\nTada's Do-It-All House\n3\n\n\n120\nGrindhouse\n3\n\n\n119\nDeath Proof\n3\n\n\n1312\nEldorado\n3\n\n\n590\nMatlock\n3\n\n\n1336\nCelebrity Ghost Stories\n3\n\n\n2210\nByplayers ~Meiwakiyaku no Mori no 100-nichikan~\n3\n\n\n530\nDrive\n2\n\n\n1342\nBreak\n2\n\n\n464\nGojoe: Spirit War Chronicle\n2\n\n\n457\nThe Private Detective Mike\n2\n\n\n528\nPermanent Vacation\n2\n\n\n1396\nOn Air with Ryan Seacrest\n2\n\n\n2943\nDoing Time\n2\n\n\n2131\nEngine Sentai Go-Onger\n2\n\n\n2915\nTales of the Bizarre: 2009 Spring Special\n2\n\n\n2169\nYakuza Ladies: Burning Desire\n2\n\n\n549\nThe Monster Hunter\n2\n\n\n557\nTrue Crime\n2\n\n\n444\nDead End Run\n2\n\n\n471\nRound About Midnight\n2\n\n\n2163\nStrait Jacket\n2\n\n\n1362\nLast Hour\n2\n\n\n493\nCheck\n2\n\n\n\n\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared = merged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\nmerged_shared\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n1\n\n\n1\nUma Thurman\nOh, Canada\n1\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n1\n\n\n3\nUma Thurman\nAnita\n1\n\n\n4\nUma Thurman\nThe Kill Room\n1\n\n\n...\n...\n...\n...\n\n\n3767\nSō Yamanaka\nHush!\n1\n\n\n3768\nSō Yamanaka\nGips\n1\n\n\n3769\nSō Yamanaka\nNot Forgotten\n1\n\n\n3770\nSō Yamanaka\nThe Exam\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns\n\n\n\n\n\n\n\n\n\nThis bar graph represents the amount of shared actors for the movies of all actors of Kill Bill Vol. 1. I am only interested in the first/top 25 values because these contain the movies with the most amount of shared actors. For example, movies other than Kill Bill Vol. 1 and Kill Bill Vol. 2 barely or do not at all have any shared actors, so they are not worth seeing.\nNote: I am using the shared_actors dataframe for this example as this dataframe eliminates repeats of the movie name that would have occurred if the actor names were involved.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.histogram(shared_actors.head(25),\n                                   x = \"Movie or TV Name\",\n                                   y = \"Number of Shared Actors\",\n                                   title = \"Number of Shared Actors For Each Movie\")\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\nObviously, Kill Bill Vol. 1 would contain the most amount of shared actors as every actor in the dataset would be considered a shared actor here. However, surprisingly, Kill Bill: The Whole Bloody Affair also has the same amount of shared actors, which suggests that all characters in Kill Bill Vol. 1 are also in this movie.\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill: Vol. 1 has appeared in throughout their entire acting careers.\nNote: I am using the merged_shared dataframe for this example because now I am interested in the occurrence of that actor’s name in the original dataset\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.histogram(merged_shared,\n                                   x = \"Actor\",\n                                   barmode = \"stack\",\n                                   title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/hw2_resubmit.html#part-1-initial-steps",
    "href": "posts/bruin/HW2_copy/hw2_resubmit.html#part-1-initial-steps",
    "title": "HW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping",
    "section": "",
    "text": "My scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browser that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper."
  },
  {
    "objectID": "posts/bruin/HW2_copy/hw2_resubmit.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "href": "posts/bruin/HW2_copy/hw2_resubmit.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "title": "HW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping",
    "section": "",
    "text": "A file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThe first method begins on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\nWhat is a Scrapy request?\nScrapy uses Requst objects, as well as Response, for crawling websites. Request objects are generated in the spiders and executed by the Downloader to create a Response.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis second method starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\nA for loop is used to go through each actor within the “Cast” section and arrives at each actor’s url to their personal page.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThe third method starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone"
  },
  {
    "objectID": "posts/bruin/HW2_copy/hw2_resubmit.html#part-3-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show-kill-bill-vol.-1",
    "href": "posts/bruin/HW2_copy/hw2_resubmit.html#part-3-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show-kill-bill-vol.-1",
    "title": "HW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping",
    "section": "",
    "text": "The following dataframes containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Vol. 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill Vol. 1 actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# sort by movies with the most to least number of shared actors\nshared_actors = shared_actors.sort_values(by = \"Number of Shared Actors\",\nascending = False)\n\nshared_actors.head(50)\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n60\nKill Bill: Vol. 1\n38\n\n\n36\nKill Bill: The Whole Bloody Affair\n38\n\n\n57\nKill Bill: Vol. 2\n32\n\n\n14\nQT8: The First Eight\n7\n\n\n160\nIchi the Killer\n5\n\n\n496\nYonimo Kimyou na Monogatari Tokubetsuhen\n5\n\n\n2580\nHero\n4\n\n\n59\nThe Making of 'Kill Bill Vol. 1'\n4\n\n\n2208\nThe Supporting Actors in Byplaywood\n4\n\n\n56\nThe Making of 'Kill Bill Vol. 2'\n4\n\n\n76\nThe View\n4\n\n\n2511\nSignal\n3\n\n\n144\nSolitary Gourmet\n3\n\n\n94\nLIVE with Kelly and Mark\n3\n\n\n1890\nSwing Girls\n3\n\n\n1891\nThe Taste of Tea\n3\n\n\n1150\nCSI: Crime Scene Talks\n3\n\n\n1122\nBlade of the Immortal\n3\n\n\n1368\nBeing Michael Madsen\n3\n\n\n454\nAIBOU: Tokyo Detective Duo\n3\n\n\n165\nDjango Unchained\n3\n\n\n448\nAlive\n3\n\n\n447\n9 Souls\n3\n\n\n438\nGodzilla: Final Wars\n3\n\n\n2002\nCackling Kitarou\n3\n\n\n198\nThe X-Files\n3\n\n\n211\nTada's Do-It-All House\n3\n\n\n120\nGrindhouse\n3\n\n\n119\nDeath Proof\n3\n\n\n1312\nEldorado\n3\n\n\n590\nMatlock\n3\n\n\n1336\nCelebrity Ghost Stories\n3\n\n\n2210\nByplayers ~Meiwakiyaku no Mori no 100-nichikan~\n3\n\n\n530\nDrive\n2\n\n\n1342\nBreak\n2\n\n\n464\nGojoe: Spirit War Chronicle\n2\n\n\n457\nThe Private Detective Mike\n2\n\n\n528\nPermanent Vacation\n2\n\n\n1396\nOn Air with Ryan Seacrest\n2\n\n\n2943\nDoing Time\n2\n\n\n2131\nEngine Sentai Go-Onger\n2\n\n\n2915\nTales of the Bizarre: 2009 Spring Special\n2\n\n\n2169\nYakuza Ladies: Burning Desire\n2\n\n\n549\nThe Monster Hunter\n2\n\n\n557\nTrue Crime\n2\n\n\n444\nDead End Run\n2\n\n\n471\nRound About Midnight\n2\n\n\n2163\nStrait Jacket\n2\n\n\n1362\nLast Hour\n2\n\n\n493\nCheck\n2\n\n\n\n\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared = merged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\nmerged_shared\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n1\n\n\n1\nUma Thurman\nOh, Canada\n1\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n1\n\n\n3\nUma Thurman\nAnita\n1\n\n\n4\nUma Thurman\nThe Kill Room\n1\n\n\n...\n...\n...\n...\n\n\n3767\nSō Yamanaka\nHush!\n1\n\n\n3768\nSō Yamanaka\nGips\n1\n\n\n3769\nSō Yamanaka\nNot Forgotten\n1\n\n\n3770\nSō Yamanaka\nThe Exam\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns"
  },
  {
    "objectID": "posts/bruin/HW2_copy/hw2_resubmit.html#data-visualization",
    "href": "posts/bruin/HW2_copy/hw2_resubmit.html#data-visualization",
    "title": "HW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping",
    "section": "",
    "text": "This bar graph represents the amount of shared actors for the movies of all actors of Kill Bill Vol. 1. I am only interested in the first/top 25 values because these contain the movies with the most amount of shared actors. For example, movies other than Kill Bill Vol. 1 and Kill Bill Vol. 2 barely or do not at all have any shared actors, so they are not worth seeing.\nNote: I am using the shared_actors dataframe for this example as this dataframe eliminates repeats of the movie name that would have occurred if the actor names were involved.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.histogram(shared_actors.head(25),\n                                   x = \"Movie or TV Name\",\n                                   y = \"Number of Shared Actors\",\n                                   title = \"Number of Shared Actors For Each Movie\")\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\nObviously, Kill Bill Vol. 1 would contain the most amount of shared actors as every actor in the dataset would be considered a shared actor here. However, surprisingly, Kill Bill: The Whole Bloody Affair also has the same amount of shared actors, which suggests that all characters in Kill Bill Vol. 1 are also in this movie.\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill: Vol. 1 has appeared in throughout their entire acting careers.\nNote: I am using the merged_shared dataframe for this example because now I am interested in the occurrence of that actor’s name in the original dataset\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.histogram(merged_shared,\n                                   x = \"Actor\",\n                                   barmode = \"stack\",\n                                   title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "emilyBlog",
    "section": "",
    "text": "HW5: Using Machine Learning Models in Image Classification\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW5: Using Machine Learning Models in Image Classification\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Matrix Multiplication and Jax Numpy (Heat Diffusion)\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Matrix Multiplication and JAX/Numpy\n\n\n\n\n\n\nweek 7\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW3\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW2: Data Manipulation and Visualization of My Recommendations for Top Works with Shared Actors Using Web Scraping\n\n\n\n\n\n\nweek 4\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\n\n\n\nweek 4\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nHW2\n\n\n\n\n\n\nweek 4\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEmily Shi\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nHW1\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nEmily Shi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruin/HW1/index.html",
    "href": "posts/bruin/HW1/index.html",
    "title": "HW1",
    "section": "",
    "text": "I begin by importing the necessary libaries and packages to create a database that consists of three tables: temperatures, stations, and countries. The data for these tables are extracted from csv files. The cleaning process involves some of the column names being renamed and column values adjusted accordingly. sqlite3.connect() points to the database location.\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\nfrom plotly.io import write_html\n\n\ndef prepare_df(df):\n    \"\"\"\n    Cleans the dataframe\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    \n    return(df)\n\n# creating a database in the current directory called temps.db\nwith sqlite3.connect(\"temps.db\") as conn: \n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    stations_csv = \"station-metadata.csv\"\n    stations = pd.read_csv(stations_csv)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n    countries = pd.read_csv(countries_url)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n    \n# the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\ndf.to_sql() writes to a specified table, either temperatures, stations, or countries in temps.db. Becuase the tables for the metadata in our database are from pretty small data sets, there is no need to read it in by chunks. if_exists = “replace” ensures that a new piece is added to the table each time rather than being overwritten.\n\n\n\nNow, I will perform some basic queries on the data tables by joining them based on relational information using INNER JOIN (only rows that have common characteristics similar to the intersection of two sets).\nFor example, the id of the stations table corresponds to the id of the temperatures table.\nThe WHERE parameters take the arguments passed in to correspond to the columns of the dataframe.\nInstead of using the cursor for prototyping SQL queries, here I use pd.read_sql_query(cmd, conn) as an easier approach.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns a dataframe of the temperatures at stations specified\n    by latitude, longitude, and country within a range of months and \n    years.    \n    \"\"\"\n    # creating a database in the current directory \n    # inner join (default for pd.merge): shared columns between the two sets\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE T.year &gt;= {year_begin}\n        AND T.year &lt;= {year_end}\n        AND T.month = {month}\n        AND C.name = \"{country}\"\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.rename(columns={\"Name\" : \"Country\"})\n        \n        return df\n        # the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\n\n\n\n\n\nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nThe return value for the India example has 3152 rows. The countries “Name” column has been altered to be “Country.”\n\n\n\n\nI will now address the question:\nHow does the average yearly change in temperature vary within a given country?\n\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\n\ncoef() computes an estimate of the year-over-year average change in temperature in each month at each station using linear regression. The coefficient of Year is an estimate of the yearly change in Temp when regressing Temp against Year.\n\ndef coef(data_group):\n    \"\"\"\n    Returns the first coeffecient of the linear regression model.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nmin_obs refers to the minimum number of years of data for any given station. Because I am only interested in data of at least min_obs years worth of data in the specified month, all other data should be filtered out using df.transform() plus filtering.\nAdditional keyword arguments may also be passed to the scatter_mapbox() to control the colormap and mapbox style.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Returns a figure of the estimated yearly increase in temperature\n    of the latitude against longitude in a specified country. \n    \"\"\"\n    query_climate_df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    query_data_obs = \\\n    query_climate_df[(query_climate_df.groupby(\"NAME\")[\"Year\"].transform(len)) &gt;= min_obs]\n    \n    # first coefficient of a linear regression model at that station is computed using the .apply() method\n    coefs = query_climate_df.groupby([\"NAME\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    merge_data = pd.merge(coefs, query_climate_df, on = \"NAME\")\n    merge_data[\"Estimated Yearly Increase (°C)\"] = \\\n    round(merge_data[0], 3)\n    \n    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\",\n             6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\",\n             11:\"November\", 12:\"December\"}\n    \n    # first argument must be dataframe\n    # lat and lon arguments tell px which columns contain the latitude and longitude coordinates\n    # hover_name : what appears when hovering over the plotted points\n    # height : aspect ratio\n    fig = px.scatter_mapbox(merge_data,\n                           lat = \"LATITUDE\",\n                           lon = \"LONGITUDE\",\n                           hover_name = \"NAME\",\n                           color = \"Estimated Yearly Increase (°C)\",\n                           height = 300,\n                           opacity = .2,\n                            # colorbar is centered at 0 so that the \"middle\" of the\n                            # of the colorbar corresponds to a coefficient of 0\n                           color_continuous_midpoint = 0,\n                           labels = {\"LATITUDE\", \"LONGITUDE\"},\n                           title = \\\n                           f\"\"\"\n                           Estimates of yearly increase in temperature in {months[month]} for stations in {country}, years {year_begin} - {year_end}\n                           \"\"\",\n                           **kwargs)\n    # controls which map tiles are used in the visualization and the amount of whitespace\n    fig.update_layout(margin = {\"r\":0,\"l\":0,\"b\":0,\"t\":50}, font_size = 10)\n    return fig\n\nInteractive geographic scatterplots are outputted with a point for each station colored by the estimate of the yearly change in temperature during the specified month and time at that station.\n\n\nThis plot displays the estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"India\", \n                                   year_begin = 1980, \n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\nThis plot displays the estimated yearly increases in temperature during the month of September, in the interval 2000-2010, in Brazil.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"Brazil\", \n                                   year_begin = 2000, \n                                   year_end = 2010, \n                                   month = 9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThis additional SQL query function returns a dataframe of the average temperatures at stations within 0-90 degrees latitude and 0-180 degrees longitude, which represents the regions of the Northern Hemisphere (between the equator and the North Pole), during the months of the specified year of 2020.\n\ndef query_northHemi_database(year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations within\n    0-90 degrees latitude and 0-180 degrees longitude, which represents\n    the regions of the Northern Hemisphere, during a specified year.\n    \"\"\"\n    cmd =\\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    AND T.Year = {year}\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n\n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    # sort the months by order\n    df = df.sort_values(by = \"Month\")\n\n    return df\n\n\nquery_northHemi_database(2020)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\n\n\n\n\n1386\nLOP_BURI\n14.800\n100.617\nThailand\n2020\n1\n29.4\n\n\n1823\nPARTIZANSK\n43.150\n133.017\nRussia\n2020\n1\n6.4\n\n\n1824\nPASNI\n25.290\n63.344\nPakistan\n2020\n1\n25.8\n\n\n1825\nPATIALA\n30.333\n76.467\nIndia\n2020\n1\n22.8\n\n\n1826\nPATNA\n25.600\n85.100\nIndia\n2020\n1\n24.1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2119\nSHALATIN\n23.133\n35.583\nEgypt\n2020\n9\n27.0\n\n\n590\nEL_ARISH_INTL\n31.073\n33.836\nEgypt\n2020\n9\n27.0\n\n\n2044\nSALLUM_PLATEAU\n31.567\n25.133\nEgypt\n2020\n10\n19.1\n\n\n637\nFARAFRA\n27.050\n27.983\nEgypt\n2020\n11\n16.1\n\n\n595\nEL_KHOMS\n32.633\n14.300\nLibya\n2020\n12\n15.7\n\n\n\n\n2774 rows × 7 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a facet scatterplot is created representing the average temperatures of the stations in the Northern Hemisphere during 2020. This plot involves facets separated by the month.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef northHemi_plot(year, **kwargs):\n    \"\"\"\n    Returns a figure containing a scatter plot of latitude against average temperatures at \n    each station in countries within the Northern Hemisphere during a specific year. The \n    color of each bar is distinguished by temperature while the label of each bar is \n    displayed as the station, latitude, and country. Each facet is separated by month in \n    order. \n    \"\"\"\n    query_northHemi = query_northHemi_database(2020)\n    \n    fig = px.scatter(query_northHemi, \n                     x = \"LATITUDE\",\n                     y = \"Average Temperature\",\n                     title = \\\n                     f\"\"\"\n                     Average Temperatures at Stations in the Northern Hemisphere During 2020\n                     \"\"\",\n                     color = \"Average Temperature\",\n                     hover_name = \"Country\",\n                     hover_data = [\"NAME\", \"LATITUDE\", \"Country\"],\n                     width = 700,\n                     height = 2000,\n                     opacity = 0.5,\n                     color_continuous_midpoint = 0,\n                     facet_row=\"Month\",\n                     )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig_northHemi = northHemi_plot(2020)\n\nfig_northHemi.show()\n\n\n\n\nIt can be seen that much of the available data was collected in the month of January during 2020 where it is especially evident in this month that the average temperature seemed to decrease as latitude increased. This is reasonable as latitude increases as you move closer to the poles, which have a lower temperature than regions closer to the equator (0 degrees latitude). For other months, because the data is more limited, this pattern is much less clear as some months have only one data point.\n\n\n\n\n\n\nNext, building on the previous question as the Northern Hemisphere is still in focus, now I will add an additional component: seasons. Specifically, I am interested in the summer and winter seasons, and the average temperatures associated with each station in each country in the Northern Hemisphere during the summer and winter.\nThe months of summer are June, July, and August, which can be numerically expressed as 6, 7, and 8, respectively. The months of winter are December, January, and February, which can be numerically expressed as 12, 1, and 2, respectively.\n\nsummerWinterMonths = {\"Summer\": [6,7,8], \"Winter\":[12, 1, 2]}\n\nA new SQL query function is created to reorganize the data according to the summer and winter months of a specific country and year. Only the summer and winter months are of focus for this query function.\n\ndef query_summerWinter_database(season, year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations of countries \n    still within the Northern Hemisphere during the months of a specified \n    season and year.\n    \"\"\"\n    # grab each month by the values associated with the key as inputted by the parameter\n    cmd = \\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE T.year = {year}\n    AND (T.Month = {summerWinterMonths[season][0]} \n    OR T.Month = {summerWinterMonths[season][1]}\n    OR T.Month = {summerWinterMonths[season][2]})\n    AND (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    df[\"Season\"] = season\n#     df = df.sort_values(by = \"Month\")\n#     df[\"Season\"] = list(summerWinterMonths.keys())[list(summerWinterMonths.values()).index(df.get(\"Month\"))]\n#     df[\"Season\"] = df[\"Season\"].sort_values(by = \"Month\")\n\n    return df\n\nAs an example, the average temperatures during the summer season of 2012 can be seen at each station and respective countries.\n\nquery_summerWinter_database(\"Summer\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n6\n14.1\nSummer\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n6\n17.0\nSummer\n\n\n2\nABAGNAR_QI\n43.9000\n116.0000\nChina\n2012\n7\n21.3\nSummer\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n6\n19.6\nSummer\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n6\n19.8\nSummer\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3977\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n8\n18.0\nSummer\n\n\n3978\nZUNYI\n27.7000\n106.8830\nChina\n2012\n6\n25.0\nSummer\n\n\n3979\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n6\n21.1\nSummer\n\n\n3980\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n6\n16.7\nSummer\n\n\n3981\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n6\n14.0\nSummer\n\n\n\n\n3982 rows × 8 columns\n\n\n\nOn the other hand, the following displays the average temperatures during the winter season of 2012 at each station and respective countries.\n\nquery_summerWinter_database(\"Winter\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n1\n5.4\nWinter\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n1\n2.3\nWinter\n\n\n2\nABADAN\n30.3710\n48.2280\nIran\n2012\n12\n16.2\nWinter\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n1\n-21.9\nWinter\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n1\n-23.9\nWinter\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4039\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n12\n-7.6\nWinter\n\n\n4040\nZUNYI\n27.7000\n106.8830\nChina\n2012\n1\n4.5\nWinter\n\n\n4041\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n1\n-20.1\nWinter\n\n\n4042\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n1\n-2.5\nWinter\n\n\n4043\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n1\n-33.4\nWinter\n\n\n\n\n4044 rows × 8 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a bar graph is created representing the average temperatures of the stations in each country of the Northern Hemisphere during the summer and winter seasons of 2012.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef summerWinter_plot(season, year, **kwargs):\n    \"\"\"\n    Returns a figure containing a bar graph of countries against average temperatures at each station\n    during a given season and year. The color of each bar is distinguished by temperature while the \n    label of each bar is displayed as the country, station, latitude, and month. \n    \"\"\"\n    query_summerWinter = query_summerWinter_database(season, year)\n    \n    fig = px.bar(query_summerWinter, \n                            x = \"Country\",\n                            y = \"Average Temperature\",\n                            title = \\\n                            f\"\"\"\n                            Average Temperatures in °C at Each Station in the {season} of {year} for Each Country\n                            \"\"\",\n                            color = \"Average Temperature\",\n                            color_continuous_scale='Bluered_r',\n                            hover_data = [\"Country\",\"NAME\", \"LATITUDE\", \"Month\"],\n                            )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the summer of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Summer\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has the highest average temperatures at its stations among the countries in the summer of 2012. On the other hand, countries such as Singapore and Cambodia have the lowest average temperatures at their stations compared to other countries in the summer of 2012.\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the winter of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Winter\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has some of the lowest average temperatures at its stations among the countries in the winter of 2012. On the other hand, southeast Asian countries such as Laos and Myanmar have some of the highest average temperatures at their stations compared to other countries in the winter of 2012.\nFinally closing the database:\n\nconn.close()\n\n\n\n\n\nUnfortunately, one issue with the original color map gradient (color_map = px.colors.diverging.RdGy_r) is that most of the values were either very low which were hard to see if they were associated with a very light color such as yellow. This causes the majority stations with positive temperatures to be difficult to see as positive temperatures are associated with lighter color in the scale. Therefore, I changed the the color_continuous_scale to be “Bluered_r” to be more apparent.\n\n\n\nThe query functions and their corresponding figures illustrate how datasets may be manipulated in terms of the variables they depict, the different ways of integrating their tables, and the type of plot used for display. Through these changes, large datasets can be visualized in a cohesive and organized way to answer a wide variety of complex questions. Another takeaway from the resulting visualizations is that collecting larger amounts of data that involve each variable of interest is important as this can provide a better overall picture of the trends inherent in the dataset. With less points to plot, detecting these patterns become much more difficult and therefore breeds a more vague answer to research questions."
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-1-create-a-database",
    "href": "posts/bruin/HW1/index.html#part-1-create-a-database",
    "title": "HW1",
    "section": "",
    "text": "I begin by importing the necessary libaries and packages to create a database that consists of three tables: temperatures, stations, and countries. The data for these tables are extracted from csv files. The cleaning process involves some of the column names being renamed and column values adjusted accordingly. sqlite3.connect() points to the database location.\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\nfrom plotly.io import write_html\n\n\ndef prepare_df(df):\n    \"\"\"\n    Cleans the dataframe\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    \n    return(df)\n\n# creating a database in the current directory called temps.db\nwith sqlite3.connect(\"temps.db\") as conn: \n    df_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(df_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    stations_csv = \"station-metadata.csv\"\n    stations = pd.read_csv(stations_csv)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n    countries = pd.read_csv(countries_url)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n    \n# the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\ndf.to_sql() writes to a specified table, either temperatures, stations, or countries in temps.db. Becuase the tables for the metadata in our database are from pretty small data sets, there is no need to read it in by chunks. if_exists = “replace” ensures that a new piece is added to the table each time rather than being overwritten."
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-2-write-a-query-function",
    "href": "posts/bruin/HW1/index.html#part-2-write-a-query-function",
    "title": "HW1",
    "section": "",
    "text": "Now, I will perform some basic queries on the data tables by joining them based on relational information using INNER JOIN (only rows that have common characteristics similar to the intersection of two sets).\nFor example, the id of the stations table corresponds to the id of the temperatures table.\nThe WHERE parameters take the arguments passed in to correspond to the columns of the dataframe.\nInstead of using the cursor for prototyping SQL queries, here I use pd.read_sql_query(cmd, conn) as an easier approach.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns a dataframe of the temperatures at stations specified\n    by latitude, longitude, and country within a range of months and \n    years.    \n    \"\"\"\n    # creating a database in the current directory \n    # inner join (default for pd.merge): shared columns between the two sets\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE T.year &gt;= {year_begin}\n        AND T.year &lt;= {year_end}\n        AND T.month = {month}\n        AND C.name = \"{country}\"\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.rename(columns={\"Name\" : \"Country\"})\n        \n        return df\n        # the with statement closes the connection when the block is exited (so conn.close() statement is not needed)\n\n\n\n\n\n\nquery_climate_database(db_file = \"temps.db\", \n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nThe return value for the India example has 3152 rows. The countries “Name” column has been altered to be “Country.”"
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-3-geographic-scatter-function",
    "href": "posts/bruin/HW1/index.html#part-3-geographic-scatter-function",
    "title": "HW1",
    "section": "",
    "text": "I will now address the question:\nHow does the average yearly change in temperature vary within a given country?\n\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\n\ncoef() computes an estimate of the year-over-year average change in temperature in each month at each station using linear regression. The coefficient of Year is an estimate of the yearly change in Temp when regressing Temp against Year.\n\ndef coef(data_group):\n    \"\"\"\n    Returns the first coeffecient of the linear regression model.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nmin_obs refers to the minimum number of years of data for any given station. Because I am only interested in data of at least min_obs years worth of data in the specified month, all other data should be filtered out using df.transform() plus filtering.\nAdditional keyword arguments may also be passed to the scatter_mapbox() to control the colormap and mapbox style.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Returns a figure of the estimated yearly increase in temperature\n    of the latitude against longitude in a specified country. \n    \"\"\"\n    query_climate_df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    query_data_obs = \\\n    query_climate_df[(query_climate_df.groupby(\"NAME\")[\"Year\"].transform(len)) &gt;= min_obs]\n    \n    # first coefficient of a linear regression model at that station is computed using the .apply() method\n    coefs = query_climate_df.groupby([\"NAME\"]).apply(coef)\n    coefs = coefs.reset_index()\n    \n    merge_data = pd.merge(coefs, query_climate_df, on = \"NAME\")\n    merge_data[\"Estimated Yearly Increase (°C)\"] = \\\n    round(merge_data[0], 3)\n    \n    months = {1:\"January\", 2:\"February\", 3:\"March\", 4:\"April\", 5:\"May\",\n             6:\"June\", 7:\"July\", 8:\"August\", 9:\"September\", 10:\"October\",\n             11:\"November\", 12:\"December\"}\n    \n    # first argument must be dataframe\n    # lat and lon arguments tell px which columns contain the latitude and longitude coordinates\n    # hover_name : what appears when hovering over the plotted points\n    # height : aspect ratio\n    fig = px.scatter_mapbox(merge_data,\n                           lat = \"LATITUDE\",\n                           lon = \"LONGITUDE\",\n                           hover_name = \"NAME\",\n                           color = \"Estimated Yearly Increase (°C)\",\n                           height = 300,\n                           opacity = .2,\n                            # colorbar is centered at 0 so that the \"middle\" of the\n                            # of the colorbar corresponds to a coefficient of 0\n                           color_continuous_midpoint = 0,\n                           labels = {\"LATITUDE\", \"LONGITUDE\"},\n                           title = \\\n                           f\"\"\"\n                           Estimates of yearly increase in temperature in {months[month]} for stations in {country}, years {year_begin} - {year_end}\n                           \"\"\",\n                           **kwargs)\n    # controls which map tiles are used in the visualization and the amount of whitespace\n    fig.update_layout(margin = {\"r\":0,\"l\":0,\"b\":0,\"t\":50}, font_size = 10)\n    return fig\n\nInteractive geographic scatterplots are outputted with a point for each station colored by the estimate of the yearly change in temperature during the specified month and time at that station.\n\n\nThis plot displays the estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"India\", \n                                   year_begin = 1980, \n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\n\n\nThis plot displays the estimated yearly increases in temperature during the month of September, in the interval 2000-2010, in Brazil.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\", \n                                   country = \"Brazil\", \n                                   year_begin = 2000, \n                                   year_end = 2010, \n                                   month = 9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\nfig.show()"
  },
  {
    "objectID": "posts/bruin/HW1/index.html#part-4-creating-two-more-sql-query-functions-corresponding-figures",
    "href": "posts/bruin/HW1/index.html#part-4-creating-two-more-sql-query-functions-corresponding-figures",
    "title": "HW1",
    "section": "",
    "text": "This additional SQL query function returns a dataframe of the average temperatures at stations within 0-90 degrees latitude and 0-180 degrees longitude, which represents the regions of the Northern Hemisphere (between the equator and the North Pole), during the months of the specified year of 2020.\n\ndef query_northHemi_database(year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations within\n    0-90 degrees latitude and 0-180 degrees longitude, which represents\n    the regions of the Northern Hemisphere, during a specified year.\n    \"\"\"\n    cmd =\\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    AND T.Year = {year}\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n\n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    # sort the months by order\n    df = df.sort_values(by = \"Month\")\n\n    return df\n\n\nquery_northHemi_database(2020)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\n\n\n\n\n1386\nLOP_BURI\n14.800\n100.617\nThailand\n2020\n1\n29.4\n\n\n1823\nPARTIZANSK\n43.150\n133.017\nRussia\n2020\n1\n6.4\n\n\n1824\nPASNI\n25.290\n63.344\nPakistan\n2020\n1\n25.8\n\n\n1825\nPATIALA\n30.333\n76.467\nIndia\n2020\n1\n22.8\n\n\n1826\nPATNA\n25.600\n85.100\nIndia\n2020\n1\n24.1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2119\nSHALATIN\n23.133\n35.583\nEgypt\n2020\n9\n27.0\n\n\n590\nEL_ARISH_INTL\n31.073\n33.836\nEgypt\n2020\n9\n27.0\n\n\n2044\nSALLUM_PLATEAU\n31.567\n25.133\nEgypt\n2020\n10\n19.1\n\n\n637\nFARAFRA\n27.050\n27.983\nEgypt\n2020\n11\n16.1\n\n\n595\nEL_KHOMS\n32.633\n14.300\nLibya\n2020\n12\n15.7\n\n\n\n\n2774 rows × 7 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a facet scatterplot is created representing the average temperatures of the stations in the Northern Hemisphere during 2020. This plot involves facets separated by the month.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef northHemi_plot(year, **kwargs):\n    \"\"\"\n    Returns a figure containing a scatter plot of latitude against average temperatures at \n    each station in countries within the Northern Hemisphere during a specific year. The \n    color of each bar is distinguished by temperature while the label of each bar is \n    displayed as the station, latitude, and country. Each facet is separated by month in \n    order. \n    \"\"\"\n    query_northHemi = query_northHemi_database(2020)\n    \n    fig = px.scatter(query_northHemi, \n                     x = \"LATITUDE\",\n                     y = \"Average Temperature\",\n                     title = \\\n                     f\"\"\"\n                     Average Temperatures at Stations in the Northern Hemisphere During 2020\n                     \"\"\",\n                     color = \"Average Temperature\",\n                     hover_name = \"Country\",\n                     hover_data = [\"NAME\", \"LATITUDE\", \"Country\"],\n                     width = 700,\n                     height = 2000,\n                     opacity = 0.5,\n                     color_continuous_midpoint = 0,\n                     facet_row=\"Month\",\n                     )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig_northHemi = northHemi_plot(2020)\n\nfig_northHemi.show()\n\n\n\n\nIt can be seen that much of the available data was collected in the month of January during 2020 where it is especially evident in this month that the average temperature seemed to decrease as latitude increased. This is reasonable as latitude increases as you move closer to the poles, which have a lower temperature than regions closer to the equator (0 degrees latitude). For other months, because the data is more limited, this pattern is much less clear as some months have only one data point.\n\n\n\n\n\n\nNext, building on the previous question as the Northern Hemisphere is still in focus, now I will add an additional component: seasons. Specifically, I am interested in the summer and winter seasons, and the average temperatures associated with each station in each country in the Northern Hemisphere during the summer and winter.\nThe months of summer are June, July, and August, which can be numerically expressed as 6, 7, and 8, respectively. The months of winter are December, January, and February, which can be numerically expressed as 12, 1, and 2, respectively.\n\nsummerWinterMonths = {\"Summer\": [6,7,8], \"Winter\":[12, 1, 2]}\n\nA new SQL query function is created to reorganize the data according to the summer and winter months of a specific country and year. Only the summer and winter months are of focus for this query function.\n\ndef query_summerWinter_database(season, year):\n    \"\"\"\n    Returns a dataframe of the average temperatures at stations of countries \n    still within the Northern Hemisphere during the months of a specified \n    season and year.\n    \"\"\"\n    # grab each month by the values associated with the key as inputted by the parameter\n    cmd = \\\n    f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, ROUND(AVG(T.Temp), 1) \"Average Temperature\"\n    FROM temperatures T\n    INNER JOIN stations S ON T.id = S.id\n    INNER JOIN countries C on SUBSTR(T.id, 1, 2) = C.\"FIPS 10-4\"\n    WHERE T.year = {year}\n    AND (T.Month = {summerWinterMonths[season][0]} \n    OR T.Month = {summerWinterMonths[season][1]}\n    OR T.Month = {summerWinterMonths[season][2]})\n    AND (S.LATITUDE &gt; 0 AND S.LATITUDE &lt; 90) \n    AND (S.LONGITUDE &gt;= 0 AND S.LONGITUDE &lt;= 180)\n    GROUP BY S.NAME, C.Name\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns={\"Name\" : \"Country\"})\n    df[\"Season\"] = season\n#     df = df.sort_values(by = \"Month\")\n#     df[\"Season\"] = list(summerWinterMonths.keys())[list(summerWinterMonths.values()).index(df.get(\"Month\"))]\n#     df[\"Season\"] = df[\"Season\"].sort_values(by = \"Month\")\n\n    return df\n\nAs an example, the average temperatures during the summer season of 2012 can be seen at each station and respective countries.\n\nquery_summerWinter_database(\"Summer\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n6\n14.1\nSummer\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n6\n17.0\nSummer\n\n\n2\nABAGNAR_QI\n43.9000\n116.0000\nChina\n2012\n7\n21.3\nSummer\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n6\n19.6\nSummer\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n6\n19.8\nSummer\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3977\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n8\n18.0\nSummer\n\n\n3978\nZUNYI\n27.7000\n106.8830\nChina\n2012\n6\n25.0\nSummer\n\n\n3979\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n6\n21.1\nSummer\n\n\n3980\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n6\n16.7\nSummer\n\n\n3981\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n6\n14.0\nSummer\n\n\n\n\n3982 rows × 8 columns\n\n\n\nOn the other hand, the following displays the average temperatures during the winter season of 2012 at each station and respective countries.\n\nquery_summerWinter_database(\"Winter\", 2012)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nAverage Temperature\nSeason\n\n\n\n\n0\nA12_CPP\n55.3992\n3.8103\nNetherlands\n2012\n1\n5.4\nWinter\n\n\n1\nAACHEN_ORSBACH\n50.7992\n6.0250\nGermany\n2012\n1\n2.3\nWinter\n\n\n2\nABADAN\n30.3710\n48.2280\nIran\n2012\n12\n16.2\nWinter\n\n\n3\nABAG_QI\n44.0170\n114.9500\nChina\n2012\n1\n-21.9\nWinter\n\n\n4\nABAKANHAKASSKAJA\n53.7700\n91.3200\nRussia\n2012\n1\n-23.9\nWinter\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4039\nZUKOVKA\n53.5330\n33.7500\nRussia\n2012\n12\n-7.6\nWinter\n\n\n4040\nZUNYI\n27.7000\n106.8830\nChina\n2012\n1\n4.5\nWinter\n\n\n4041\nZVERINOGOLOVSKAJA\n54.4670\n64.8670\nRussia\n2012\n1\n-20.1\nWinter\n\n\n4042\nZWIESEL_AUT\n49.0330\n13.2330\nGermany\n2012\n1\n-2.5\nWinter\n\n\n4043\nZYRJANKA\n65.7300\n150.9000\nRussia\n2012\n1\n-33.4\nWinter\n\n\n\n\n4044 rows × 8 columns\n\n\n\n\n\n\nUsing the SQL query function created above, a bar graph is created representing the average temperatures of the stations in each country of the Northern Hemisphere during the summer and winter seasons of 2012.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfrom plotly import express as px\n\ndef summerWinter_plot(season, year, **kwargs):\n    \"\"\"\n    Returns a figure containing a bar graph of countries against average temperatures at each station\n    during a given season and year. The color of each bar is distinguished by temperature while the \n    label of each bar is displayed as the country, station, latitude, and month. \n    \"\"\"\n    query_summerWinter = query_summerWinter_database(season, year)\n    \n    fig = px.bar(query_summerWinter, \n                            x = \"Country\",\n                            y = \"Average Temperature\",\n                            title = \\\n                            f\"\"\"\n                            Average Temperatures in °C at Each Station in the {season} of {year} for Each Country\n                            \"\"\",\n                            color = \"Average Temperature\",\n                            color_continuous_scale='Bluered_r',\n                            hover_data = [\"Country\",\"NAME\", \"LATITUDE\", \"Month\"],\n                            )\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n    return fig\n\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the summer of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Summer\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has the highest average temperatures at its stations among the countries in the summer of 2012. On the other hand, countries such as Singapore and Cambodia have the lowest average temperatures at their stations compared to other countries in the summer of 2012.\n\n\n\nThe following bar graph illustrates the average temperatures at each station of the Northern Hemisphere countries during the winter of 2012.\n\nfig_summerWinter_plot = summerWinter_plot(\"Winter\", 2012)\n\nfig_summerWinter_plot.show()\n\n\n\n\nIt can be seen that Russia has some of the lowest average temperatures at its stations among the countries in the winter of 2012. On the other hand, southeast Asian countries such as Laos and Myanmar have some of the highest average temperatures at their stations compared to other countries in the winter of 2012.\nFinally closing the database:\n\nconn.close()\n\n\n\n\n\nUnfortunately, one issue with the original color map gradient (color_map = px.colors.diverging.RdGy_r) is that most of the values were either very low which were hard to see if they were associated with a very light color such as yellow. This causes the majority stations with positive temperatures to be difficult to see as positive temperatures are associated with lighter color in the scale. Therefore, I changed the the color_continuous_scale to be “Bluered_r” to be more apparent.\n\n\n\nThe query functions and their corresponding figures illustrate how datasets may be manipulated in terms of the variables they depict, the different ways of integrating their tables, and the type of plot used for display. Through these changes, large datasets can be visualized in a cohesive and organized way to answer a wide variety of complex questions. Another takeaway from the resulting visualizations is that collecting larger amounts of data that involve each variable of interest is important as this can provide a better overall picture of the trends inherent in the dataset. With less points to plot, detecting these patterns become much more difficult and therefore breeds a more vague answer to research questions."
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html",
    "title": "HW2",
    "section": "",
    "text": "This activity aims to find the works of actors in a certain movie only under a specific category: “Acting.” Some actors may also hold other positions on other teams, such as a stunt double on “Crew” or an assistant director on “Production,” but this activity is only interested in the “Acting” positions of the “actors” within a specific movie.\nIn the following tutorial, I will be demonstrating the webscraping process using the TMDB sites of two movies: first from the example movie, Harry Potter and the Philosopher’s Stone, and later from my favorite movie, Kill Bill: Vol. 1.\n\n\n\n\nMy scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper.\n\n\n\n\n\nA file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\n\n\n\n\n\n\nThis is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns\n\n\n\n\n\n\n\n\n\nThis bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-1-initial-steps",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-1-initial-steps",
    "title": "HW2",
    "section": "",
    "text": "My scraper will look at all of the actors in this movie as well as all of the other movies/TV shows that they worked on. Writing my scraper then requires me to inspect the individual HTML elements using the Developer Tools of my browswer that contain the names and works that I am looking for.\nUpon landing on the TMDB page for this movie, I first save the url.\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nI then scroll down to the Full Cast & Crew link, click on it, and then scroll to the Cast section. When I click on the portrait of one of the actors, I am taken to that specific actor’s TMDB page. Each actor’s page contains the information that I am interested in, which is the movies/TV shows that that actor worked on specifically under the “Acting” category.\n\n\n\nIn the terminal, run the commands:\nconda activate PIC16B-24W scrapy startproject TMDB_scraper cd TMDB_scraper\n\n\nIn the settings.py file, add the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis line prevents the scraper from downloading an excessive amount of data during my testing phase. Later, this line will be removed.\nAdditionally:\nscrapy shell -s USER_AGENT=‘Scrapy/2.8.0 (+https://scrapy.org)’ https://www.themoviedb.org/…\nThis changes the user agent on scrapy shell so that the website does not block my scraper."
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#part-2-tmdbspider-scraper-for-harry-potter-example",
    "title": "HW2",
    "section": "",
    "text": "A file called tmdb_spider.py will be created inside the spiders directory that includes the following lines.\n\nimport scrapy\n\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nBy later providing a subdirectory (subdir) on the TMDB website that is specific to Kill Bill: Vol. 1, the spider will be able to run through that movie.\nThere are 3 parsing functions that belong to the TmdbSpider class.\n\n\nThis function starts on the movie page and then navigates to the Full Cast & Crew page through cast_crew_url. Once on that page, the parse_full_credits() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse(self, response):\n            \"\"\"\n            Parses the starting url page for the favorite movie by\n            first navigating to the Full Cast and Crew Page from\n            the movie page.\n\n            Does not return any data.\n            \"\"\"\n            # starting url plus cast at end\n            cast_crew_url = response.url + \"/cast\"\n\n            # ask scrapy to visit the cast_crew link and once it gets there,\n            # call self.parse_full_credits() recursively\n            yield scrapy.Request(cast_crew_url, callback=self.parse_full_credits)\n\n\n\n\nThis function starts on the Full Cast & Crew page and yields a Scrapy request for each actor’s personal page (actors only, no crew members). Once on that page, the parse_actor_page() function is called by a callback argument to a yielded Scrapy request.\n\ndef parse_full_credits(self, response):\n        \"\"\"\n        Starts on the Full Cast & Crew page\n        Yields a scrapy Request for the page of each actor listed on the \n        page (not including the crew members).\n\n        Does not return any data.\n        \"\"\"\n        # list of links for the actors' pages as displayed in the Full Cast and Crew\n        main_url = \"https://www.themoviedb.org\"\n\n        # [0]: category being \"Cast\" \n        for link in response.css(\"ol.people.credits\")[0].css(\"li\"):\n            # gets the link of that actor's page\n            actor_page = link.css(\"a::attr(href)\").get()\n            actor_url = f'{main_url}{actor_page}'\n            yield scrapy.Request(actor_url, callback=self.parse_actor_page)\n\n\n\n\nThis function starts on the individual actor’s page and yields a dictionary containing the name of the actor (keys) and the movie/TV show name the actor was in (values). Specifically, we are only interested in the works under the “Acting” category of that actor. As a reuslt, one actor’s name could appear many times if that actor had been in many movies/TV shows.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        On each individual's page\n        Yields a dictionary of the actor name (key) and that actor's works\n        under only the \"Acting\" category \n        \"\"\"\n        actor_name_CSS = response.css(\"h2.title a::text\").get()\n        # checks for the h3 category of \"Acting\" regardless of index number because \n        # some of the individuals such as David Holmes does not have \"Acting\" as \n        # their first category unlike most others\n        if response.css('h3.zero:contains(\"Acting\")'):\n            index = 0;\n        elif response.css('h3.one:contains(\"Acting\")'):\n            index = 1;\n        elif response.css('h3.two:contains(\"Acting\")'):\n            index = 2;\n        # gets the works of only the \"Acting\" category\n        movie_or_TV_CSS = response.css(\"table.card.credits\")[index].css(\"a.tooltip bdi::text\")\n        for each_work in movie_or_TV_CSS:\n            movie_or_TV_name = each_work.get()\n            yield {\"actor\" : actor_name_CSS, \n                   \"movie_or_TV_name\" : movie_or_TV_name}\n\n\n\n\nOnce all functions are correctly implemented, the following command creates a results.csv that contains all actors in Harry Potter and the Philosopher’s Stone and their “Acting” works in key-value pairs.\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#sorted-list-with-top-moviestv-shows-that-share-actors-with-my-favorite-movietv-show",
    "title": "HW2",
    "section": "",
    "text": "This is a sorted list containing the top movies and TV shoes that share actors with my favorite movie: Kill Bill Volume 1. It has two columns: the names of the movies and the number of shared actors.\nUsing the functions used to scrape the Harry Potter movie, I will generate a new file for the Kill Bill actors and their works.\n\n\nscrapy crawl tmdb_spider -o resultsFavMovie.csv -a subdir=24-kill-bill-vol-1\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n# read in csv file for Kill Bill actors + their movies/shows\nfilename = \"resultsFavMovie.csv\"\nkillBill_actorsMovies = pd.read_csv(filename)\n\n\n# rename the dictionary\nrename_dict = {\"actor\" : \"Actor\",\n               \"movie_or_TV_name\" : \"Movie or TV Name\"}\nkillBill_actorsMovies.rename(columns = rename_dict, inplace = True)\n\n# displaying the dataframe with renamed columns\nkillBill_actorsMovies\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\n\n\n\n\n0\nUma Thurman\nThe Old Guard 2\n\n\n1\nUma Thurman\nOh, Canada\n\n\n2\nUma Thurman\nTau Ceti Foxtrot\n\n\n3\nUma Thurman\nAnita\n\n\n4\nUma Thurman\nThe Kill Room\n\n\n...\n...\n...\n\n\n3767\nSō Yamanaka\nNot Forgotten\n\n\n3768\nSō Yamanaka\nThe Exam\n\n\n3769\nSō Yamanaka\nPing Pong Bath Station\n\n\n3770\nSō Yamanaka\nYonimo Kimyou na Monogatari Tokubetsuhen\n\n\n3771\nSō Yamanaka\nKamen Rider\n\n\n\n\n3772 rows × 2 columns\n\n\n\nNow, I will create a dictionary containing each movie as the key and the number of times that movie appears in the killBill_actorsMovies dataframe. The frequency represents the number of shared actors in the movie because the same movie name appearing again means another actor is in that movie as well.\n\n# initializing the dictionary containing the movie name and its\n# frequency of occurence\nmovie_freq_dict = {}\n\n# loop through the movie names and increase the count when the movie\n# name appears again\nfor each_movie in killBill_actorsMovies[\"Movie or TV Name\"]:\n    # if the movie is NOT already in the dictionary, add it\n    if movie_freq_dict.get(each_movie) == None:\n        movie_freq_dict[each_movie] = 1\n    # else if the movie is already in the dictionary, increase the \n    # count when it reappears\n    else:\n        movie_freq_dict[each_movie] += 1\n\n\n# create a new dataframe that takes in the movie names as keys and the\n# frequency of occurence for each movie as their values\nshared_actors = pd.DataFrame({\"Movie or TV Name\" : movie_freq_dict.keys(),\n                             \"Number of Shared Actors\" : movie_freq_dict.values()})\n\n# find the row with \"Kill Bill: Vol. 1\" and move it to the top row\n# killBill_frontOfDF = shared_actors[shared_actors[\"Movie or TV Name\"].str.contains(\"Kill Bill: Vol. 1\")]\n# killBill_frontOfDF\n\nshared_actors\n\n\n\n\n\n\n\n\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n0\nThe Old Guard 2\n1\n\n\n1\nOh, Canada\n1\n\n\n2\nTau Ceti Foxtrot\n1\n\n\n3\nAnita\n1\n\n\n4\nThe Kill Room\n1\n\n\n...\n...\n...\n\n\n3344\nHush!\n1\n\n\n3345\nGips\n1\n\n\n3346\nNot Forgotten\n1\n\n\n3347\nThe Exam\n1\n\n\n3348\nPing Pong Bath Station\n1\n\n\n\n\n3349 rows × 2 columns\n\n\n\nTo make the dataframe more detailed, it would be nice to see which actors from Kill Bill are in the same movies together. This would mean merging the two dataframes and desginating an additional column for the names of the Kill Bill actors that are also in these other movies together. These are the “shared actors” of each movie.\n\nmerged_shared = pd.merge(killBill_actorsMovies, shared_actors,\n                        on = \"Movie or TV Name\")\n\n# sort by movies with the most to least number of shared actors\nmerged_shared.sort_values(by = \"Number of Shared Actors\", \n                          ascending = False)\n\n\n\n\n\n\n\n\nActor\nMovie or TV Name\nNumber of Shared Actors\n\n\n\n\n176\nVivica A. Fox\nKill Bill: Vol. 1\n38\n\n\n54\nChiaki Kuriyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n61\nIssey Takahashi\nKill Bill: The Whole Bloody Affair\n38\n\n\n60\nYoshiko Yamaguchi\nKill Bill: The Whole Bloody Affair\n38\n\n\n59\nRonnie Yoshiko Fujiyama\nKill Bill: The Whole Bloody Affair\n38\n\n\n...\n...\n...\n...\n\n\n1524\nMichael Madsen\nWelcome to Acapulco\n1\n\n\n1525\nMichael Madsen\nTrading Paint\n1\n\n\n1526\nMichael Madsen\nHangover in Death Valley\n1\n\n\n1527\nMichael Madsen\nDead On Time\n1\n\n\n3771\nSō Yamanaka\nPing Pong Bath Station\n1\n\n\n\n\n3772 rows × 3 columns"
  },
  {
    "objectID": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#data-visualization",
    "href": "posts/bruin/HW2_copy/tmdb_spider-Copy1.html#data-visualization",
    "title": "HW2",
    "section": "",
    "text": "This bar graph represents the amount of shared actors for the movies that all of the actors have been in. I am interested in only the first/top 50 movies as those have the most amount of shared actors.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared.head(50),\n                            x = \"Movie or TV Name\",\n                             y = \"Number of Shared Actors\",\n                            title = \"Number of Shared Actors For Each Movie\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()\n\n\n\n\n\n\n\nThis bar graph represents the amount of movies that each actor in Kill Bill has appeared in throughout their entire acting careers.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nactorWorksCount_fig = px.bar(merged_shared,\n                            x = \"Actor\",\n                            title = \"Number of Movies/TV Shows For Each Actor\")\n\nactorWorksCount_fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\nactorWorksCount_fig.show()"
  },
  {
    "objectID": "posts/bruin/HW3/index.html",
    "href": "posts/bruin/HW3/index.html",
    "title": "HW3",
    "section": "",
    "text": "To create simple web pages, we begin by installing Flask.\nFlask is a Python web framework used to create and enable web apps simply.\n\n\nTo setup flask, enter the Anaconda Prompt terminal window and type in conda activate PIC16B-24W and pip install flask. Then, set the Flask environment to “development” through set FLASK_ENV=development. This allows the developer to debug by viewing the specific error messages through the terminal. When the web page is ready to be viewed, simply cd to the correct directory location and type in flask run.\n\n\n\nWithin a newly created templates\\ folder, we will create three HTML files that will be the layout for our three web pages: base.html, submit.html, and view.html.\nbase.html acts as the home page that the user first lands on upon visiting the link. It contains links to the following web pages, submit.html and view.html, that serve other functions. It also contains a content section where the messages will later appear in. The styling of the page is due to the import of style.css which adds color and changes the font.\n\n\n\n# header for HTML files\n&lt;!doctype html&gt;\n# link to a stylesheet (style.css) in the 'static' directory\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n# main title for base.html\n&lt;title&gt;{% block title %}{% endblock %} - Home Page &lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Message Center&lt;/h1&gt;\n  &lt;ul&gt;\n    # renders as a link to either submit or view messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nsubmit.html is the page where the user visits to submit a message. It extends base.html in that it has the same styling and content section where messages appear. Here, there are labels and input boxes that allow the user to input a message and handle (username). Lastly, there is a button to submit the form, which sends the information to the data table within the database to then be viewed in view.html.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit a Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n    &lt;label for=\"message\"&gt;Please write a message:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;\n    &lt;label for=\"handle\"&gt;What is your handle or name?&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;\n    &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n\n{% endblock %}\n\nview.html is the page where the user visits to view submitted messages. It extends base.html in that it has the same styling and content section where messages appear. Here, within the “content” block, a for loop retrieves the information in the database to then display the handle and corresponding message in a dictionary-like format.\nIt takes advantage of the fact that Jinja tags support indexing of objects, where message[0] contains the handle and message[1] contains the message.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block title %}\nView Messages\n{% endblock %}\n\n{% block content %}\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n# rand_mess passed in by view() function in app.py\n{% for message in rand_mess %}\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\n\n\n\n\n\n\nget_message_db() creates the database and checks if there is a database called message_db in the g attribute of the app through the try block. If that is not found, within the except block, a connection that is an attribute of g to the database must be made. Additionally, the table messages needs to be checked if it exists in the database or not. If not, create it using the SQL command CREATE TABLE IF NOT EXISTS and ensure that there is both a handle and message column (both being of text type).\nLastly, this function returns the g.message_db connection. import sqlite3 is necessary for the creation of the database.\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                              handle TEXT,\n                              message TEXT)''')\n        g.message_db.commit()\n        return g.message_db\n\n\n\n\ninsert_message() extracts the message and handle from request. Then, it assigns the inputted message and handle to variables to then be inserted into the datatable under their corresponding columns. Lastly, the database connnection is closed.\n\ndef insert_message(request):\n    message = request.form['message']\n    handle = request.form['handle']\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # SQL query to insert the message into the 'messages' table\n    insert_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    \n    cursor = db.cursor()\n    \n    # Execute the SQL query with the handle and message as parameters\n    cursor.execute(insert_query, (handle, message))\n    \n    # Commit the changes to the database\n    db.commit()\n    db.close()\n\n\n\n\nsubmit allows the submitted information to be entered into the database. When a submission has been made, the page reloads and allows for a new submission to be made with empty entry fields.\nThe @app.route() line is necessary for directing the webpage to that particular rendered html page within the url. In this case, the main url plus /submit at the end will land the user on the rendered submit.html page.\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    # POST used to send data to a server to create/update\n    if request.method == 'POST':\n        # this function handles inserting a user message into the db\n        insert_message(request)\n        # render submit.html \n        return render_template('submit.html')\n    else:\n        # if nothing is posted, just render the same page again\n        return render_template('submit.html')\n\n\n\n\nrandom_messages(n) takes in a number of messages as a parameter. It opens a database and selects the handle and corresponding message by random and returns that information as a variable.\n\ndef random_messages(n):\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # Create a cursor object to execute SQL commands\n    cursor = db.cursor()    \n    \n    # SQL query to insert the message into the 'messages' table\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))\n    messages = cursor.fetchall()\n   \n    # Commit the changes to the database\n    db.commit()\n    \n    # close the db\n    db.close()\n    \n    return messages\n\n\n\n\nview calls random_messages with a parameter of five, which would be the number of random messages in the collection returned. It then sends the returned information, being the messages and handles, as a variable to view.html to then be viewed.\n\n@app.route('/view')\ndef view():\n    # grabs 5 (or less) random messages \n    rand_mess = random_messages(5)\n    # passes the messages as an argument to render_template()\n    return render_template('view.html', rand_mess = rand_mess)\n\n\n\n\n\nLastly, to add some style to our webpage, we will change the font and incorporate color. The font is changed to “Courier New” of the “Lucida Console” font-family, while the overall background color is changed to #CD5C5C which resembles a pretty pinkish-red. The background color of the text is changed to #FFFFF0, which is a creamy white color.\n\nbody {\n    font-family: \"Lucida Console\", \"Courier New\", monospace;\n    background-color: #CD5C5C;\n}\nh1 {\n    color: #FFFFF0;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #FFFFF0;\n    padding: 10px;\n    margin-bottom: 5px;\n}\n\n\n\n\nUpon landing on the url, the first page is the rendered base.html. Here, you see the links to submit and view messages.\n\n\n\nmainpage.png\n\n\nAfter clicking the link to submit a message, you land on the rendered submit.html where you are able to fill in the forms to write a message and give a corresponding name.\n\n\n\nsubmitmessage.png\n\n\n\n\n\nwrittenmessage.png\n\n\nAfter submitting a few messages, clicking on the link to view messages takes you to the rendered view.html. Here, you can view the submitted messages (in random order).\n\n\n\nviewmessages.png\n\n\nThat’s all! Hope you enjoyed this short tutorial!\n\n\nhttps://github.com/emilyrshi/PIC16B_HW3"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#flask-setup",
    "href": "posts/bruin/HW3/index.html#flask-setup",
    "title": "HW3",
    "section": "",
    "text": "To setup flask, enter the Anaconda Prompt terminal window and type in conda activate PIC16B-24W and pip install flask. Then, set the Flask environment to “development” through set FLASK_ENV=development. This allows the developer to debug by viewing the specific error messages through the terminal. When the web page is ready to be viewed, simply cd to the correct directory location and type in flask run."
  },
  {
    "objectID": "posts/bruin/HW3/index.html#writing-the-templates-files-html",
    "href": "posts/bruin/HW3/index.html#writing-the-templates-files-html",
    "title": "HW3",
    "section": "",
    "text": "Within a newly created templates\\ folder, we will create three HTML files that will be the layout for our three web pages: base.html, submit.html, and view.html.\nbase.html acts as the home page that the user first lands on upon visiting the link. It contains links to the following web pages, submit.html and view.html, that serve other functions. It also contains a content section where the messages will later appear in. The styling of the page is due to the import of style.css which adds color and changes the font.\n\n\n\n# header for HTML files\n&lt;!doctype html&gt;\n# link to a stylesheet (style.css) in the 'static' directory\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n# main title for base.html\n&lt;title&gt;{% block title %}{% endblock %} - Home Page &lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Message Center&lt;/h1&gt;\n  &lt;ul&gt;\n    # renders as a link to either submit or view messages\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nsubmit.html is the page where the user visits to submit a message. It extends base.html in that it has the same styling and content section where messages appear. Here, there are labels and input boxes that allow the user to input a message and handle (username). Lastly, there is a button to submit the form, which sends the information to the data table within the database to then be viewed in view.html.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit a Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;form method=\"post\"&gt;\n    &lt;label for=\"message\"&gt;Please write a message:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;\n    &lt;label for=\"handle\"&gt;What is your handle or name?&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;\n    &lt;input type=\"submit\" value=\"Submit form\"&gt;\n  &lt;/form&gt;\n\n{% endblock %}\n\nview.html is the page where the user visits to view submitted messages. It extends base.html in that it has the same styling and content section where messages appear. Here, within the “content” block, a for loop retrieves the information in the database to then display the handle and corresponding message in a dictionary-like format.\nIt takes advantage of the fact that Jinja tags support indexing of objects, where message[0] contains the handle and message[1] contains the message.\n\n\n\n\n{% extends 'base.html' %}\n\n{% block title %}\nView Messages\n{% endblock %}\n\n{% block content %}\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n# rand_mess passed in by view() function in app.py\n{% for message in rand_mess %}\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#the-five-functions-of-app.py",
    "href": "posts/bruin/HW3/index.html#the-five-functions-of-app.py",
    "title": "HW3",
    "section": "",
    "text": "get_message_db() creates the database and checks if there is a database called message_db in the g attribute of the app through the try block. If that is not found, within the except block, a connection that is an attribute of g to the database must be made. Additionally, the table messages needs to be checked if it exists in the database or not. If not, create it using the SQL command CREATE TABLE IF NOT EXISTS and ensure that there is both a handle and message column (both being of text type).\nLastly, this function returns the g.message_db connection. import sqlite3 is necessary for the creation of the database.\n\ndef get_message_db():\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                              handle TEXT,\n                              message TEXT)''')\n        g.message_db.commit()\n        return g.message_db\n\n\n\n\ninsert_message() extracts the message and handle from request. Then, it assigns the inputted message and handle to variables to then be inserted into the datatable under their corresponding columns. Lastly, the database connnection is closed.\n\ndef insert_message(request):\n    message = request.form['message']\n    handle = request.form['handle']\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # SQL query to insert the message into the 'messages' table\n    insert_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    \n    cursor = db.cursor()\n    \n    # Execute the SQL query with the handle and message as parameters\n    cursor.execute(insert_query, (handle, message))\n    \n    # Commit the changes to the database\n    db.commit()\n    db.close()\n\n\n\n\nsubmit allows the submitted information to be entered into the database. When a submission has been made, the page reloads and allows for a new submission to be made with empty entry fields.\nThe @app.route() line is necessary for directing the webpage to that particular rendered html page within the url. In this case, the main url plus /submit at the end will land the user on the rendered submit.html page.\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    # POST used to send data to a server to create/update\n    if request.method == 'POST':\n        # this function handles inserting a user message into the db\n        insert_message(request)\n        # render submit.html \n        return render_template('submit.html')\n    else:\n        # if nothing is posted, just render the same page again\n        return render_template('submit.html')\n\n\n\n\nrandom_messages(n) takes in a number of messages as a parameter. It opens a database and selects the handle and corresponding message by random and returns that information as a variable.\n\ndef random_messages(n):\n    \n    # Get the database connection\n    db = get_message_db()\n    \n    # Create a cursor object to execute SQL commands\n    cursor = db.cursor()    \n    \n    # SQL query to insert the message into the 'messages' table\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))\n    messages = cursor.fetchall()\n   \n    # Commit the changes to the database\n    db.commit()\n    \n    # close the db\n    db.close()\n    \n    return messages\n\n\n\n\nview calls random_messages with a parameter of five, which would be the number of random messages in the collection returned. It then sends the returned information, being the messages and handles, as a variable to view.html to then be viewed.\n\n@app.route('/view')\ndef view():\n    # grabs 5 (or less) random messages \n    rand_mess = random_messages(5)\n    # passes the messages as an argument to render_template()\n    return render_template('view.html', rand_mess = rand_mess)"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#css-styling",
    "href": "posts/bruin/HW3/index.html#css-styling",
    "title": "HW3",
    "section": "",
    "text": "Lastly, to add some style to our webpage, we will change the font and incorporate color. The font is changed to “Courier New” of the “Lucida Console” font-family, while the overall background color is changed to #CD5C5C which resembles a pretty pinkish-red. The background color of the text is changed to #FFFFF0, which is a creamy white color.\n\nbody {\n    font-family: \"Lucida Console\", \"Courier New\", monospace;\n    background-color: #CD5C5C;\n}\nh1 {\n    color: #FFFFF0;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #FFFFF0;\n    padding: 10px;\n    margin-bottom: 5px;\n}"
  },
  {
    "objectID": "posts/bruin/HW3/index.html#my-webpage-in-action",
    "href": "posts/bruin/HW3/index.html#my-webpage-in-action",
    "title": "HW3",
    "section": "",
    "text": "Upon landing on the url, the first page is the rendered base.html. Here, you see the links to submit and view messages.\n\n\n\nmainpage.png\n\n\nAfter clicking the link to submit a message, you land on the rendered submit.html where you are able to fill in the forms to write a message and give a corresponding name.\n\n\n\nsubmitmessage.png\n\n\n\n\n\nwrittenmessage.png\n\n\nAfter submitting a few messages, clicking on the link to view messages takes you to the rendered view.html. Here, you can view the submitted messages (in random order).\n\n\n\nviewmessages.png\n\n\nThat’s all! Hope you enjoyed this short tutorial!\n\n\nhttps://github.com/emilyrshi/PIC16B_HW3"
  },
  {
    "objectID": "posts/bruin/HW5/index.html",
    "href": "posts/bruin/HW5/index.html",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "In this tutorial, we will look at how different machine learning algorithms uses image classification to distinguish between pictures of dogs and cats.\nWe will feed data through Tensorflow Datasets and classify images in Keras. Tensorflow Datasets allow us to organize operations on training, validation, and test datasets. Additionally, manipulating and augmenting our datasets allow each of the models to adopt and learn patterns more efficiently. Lastly, we will use transfer learning by incorporating pre-trained models to perform new tasks for us, which simplifies the process.\nTo run the models in this tutorial, we will enable a GPU runtime to speed the processes.\nWe begin by installing the latest version of keras in addition to importing the necessary packages.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\n\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\n\n!nvidia-smi\n\nTue Mar  5 07:09:41 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   77C    P0              32W /  70W |  11511MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nThe following creates the datasets for training, validation, and testing. The dataset is the pipeline that feeds data to a machine learning model. Datasets are used when it is not necessarily practical to load all data into memory.\n\n# all of the following provided in the instructions\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nThe dataset contains images of different sizes, so we will resize them to a fixed size of 150x150 pixels.\n\n# all of the following provided in the instructions\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nHere, we will rapidly read the data.\nbatch_size determines how many data points are gathered from the directory at once.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\n\nTo visualize our dataset, we will write a function that creates a two-row visualization. The first row of the plot contains three random pictures of cats while the second row contains three random pictures of dogs.\n\nimport random\nimport tensorflow as tf\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\n\n\ndef two_row_vis(dataset, num_rows, num_cols, num_samples):\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # take method gets a piece of the dataset, specifically one batch (or 32\n    # images with labels) from the training data\n    # skip takes the element at the nth value (being num_samples, which is\n    # a random integer between 1 and 10) away\n    for i, (images, labels) in enumerate(dataset.skip(num_samples).take(1)):\n        cat_count = 0\n        dog_count = 0\n        for image, label in zip(images, labels):\n            # row of cats\n            if label == 0 and cat_count &lt; num_cols:\n                axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[0, cat_count].set_title('Cat')\n                axes[0, cat_count].axis(\"off\")\n                cat_count += 1\n            # row of dogs\n            elif label == 1 and dog_count &lt; num_cols:\n                axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[1, dog_count].set_title('Dog')\n                axes[1, dog_count].axis(\"off\")\n                dog_count += 1\n            if cat_count == num_cols and dog_count == num_cols:\n                break\n    plt.show()\n\n\ntwo_row_vis(train_ds, 2, 3, random.randint(1,10))\n\n\n\n\n\n\n\n\n\n\n\nNow, we will create an iterator called labels_iterator.\n\n# provided in instructions\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nUsing labels_iterator, we will compute the number of images in the training data with label 0 (which represents the cats) and label 1 (which represents the dogs).\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (Cat):\", cat_count)\nprint(\"Number of images with label 1 (Dog):\", dog_count)\n\nNumber of images with label 0 (Cat): 4637\nNumber of images with label 1 (Dog): 4668\n\n\nAs we can see, there are more images of dogs than cats in this dataset.\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the most frequent label is label 1 which is 4668, the baseline model would be 4668 out of the total number of images, or (4668/(4637+4668))%, or 50.2% accuracy."
  },
  {
    "objectID": "posts/bruin/HW5/index.html#part-1-loading-packages-and-obtaining-data",
    "href": "posts/bruin/HW5/index.html#part-1-loading-packages-and-obtaining-data",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "In this tutorial, we will look at how different machine learning algorithms uses image classification to distinguish between pictures of dogs and cats.\nWe will feed data through Tensorflow Datasets and classify images in Keras. Tensorflow Datasets allow us to organize operations on training, validation, and test datasets. Additionally, manipulating and augmenting our datasets allow each of the models to adopt and learn patterns more efficiently. Lastly, we will use transfer learning by incorporating pre-trained models to perform new tasks for us, which simplifies the process.\nTo run the models in this tutorial, we will enable a GPU runtime to speed the processes.\nWe begin by installing the latest version of keras in addition to importing the necessary packages.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\n\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\n\n!nvidia-smi\n\nTue Mar  5 07:09:41 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   77C    P0              32W /  70W |  11511MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nThe following creates the datasets for training, validation, and testing. The dataset is the pipeline that feeds data to a machine learning model. Datasets are used when it is not necessarily practical to load all data into memory.\n\n# all of the following provided in the instructions\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nThe dataset contains images of different sizes, so we will resize them to a fixed size of 150x150 pixels.\n\n# all of the following provided in the instructions\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nHere, we will rapidly read the data.\nbatch_size determines how many data points are gathered from the directory at once.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/bruin/HW5/index.html#working-with-datasets",
    "href": "posts/bruin/HW5/index.html#working-with-datasets",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "To visualize our dataset, we will write a function that creates a two-row visualization. The first row of the plot contains three random pictures of cats while the second row contains three random pictures of dogs.\n\nimport random\nimport tensorflow as tf\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\n\n\ndef two_row_vis(dataset, num_rows, num_cols, num_samples):\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # take method gets a piece of the dataset, specifically one batch (or 32\n    # images with labels) from the training data\n    # skip takes the element at the nth value (being num_samples, which is\n    # a random integer between 1 and 10) away\n    for i, (images, labels) in enumerate(dataset.skip(num_samples).take(1)):\n        cat_count = 0\n        dog_count = 0\n        for image, label in zip(images, labels):\n            # row of cats\n            if label == 0 and cat_count &lt; num_cols:\n                axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[0, cat_count].set_title('Cat')\n                axes[0, cat_count].axis(\"off\")\n                cat_count += 1\n            # row of dogs\n            elif label == 1 and dog_count &lt; num_cols:\n                axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n                axes[1, dog_count].set_title('Dog')\n                axes[1, dog_count].axis(\"off\")\n                dog_count += 1\n            if cat_count == num_cols and dog_count == num_cols:\n                break\n    plt.show()\n\n\ntwo_row_vis(train_ds, 2, 3, random.randint(1,10))"
  },
  {
    "objectID": "posts/bruin/HW5/index.html#check-label-frequencies",
    "href": "posts/bruin/HW5/index.html#check-label-frequencies",
    "title": "HW5: Using Machine Learning Models in Image Classification",
    "section": "",
    "text": "Now, we will create an iterator called labels_iterator.\n\n# provided in instructions\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nUsing labels_iterator, we will compute the number of images in the training data with label 0 (which represents the cats) and label 1 (which represents the dogs).\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (Cat):\", cat_count)\nprint(\"Number of images with label 1 (Dog):\", dog_count)\n\nNumber of images with label 0 (Cat): 4637\nNumber of images with label 1 (Dog): 4668\n\n\nAs we can see, there are more images of dogs than cats in this dataset.\nThe baseline machine learning model is the model that always guesses the most frequent label. Because the most frequent label is label 1 which is 4668, the baseline model would be 4668 out of the total number of images, or (4668/(4637+4668))%, or 50.2% accuracy."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]